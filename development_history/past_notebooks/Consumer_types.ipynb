{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lexaun-chen/STAT-4830-Group-Project/blob/main/notebooks/Consumer_types.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87305e2c",
      "metadata": {
        "id": "87305e2c"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.init as init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf29n9VHKRkM",
        "outputId": "5a8504ae-38d1-43f9-847b-360390c2b375"
      },
      "id": "sf29n9VHKRkM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986d02f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "986d02f2",
        "outputId": "7b45f02e-c298-4373-8156-17cfc9f77b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU availability and set the device accordingly\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU available:\", device)\n",
        "    torch.cuda.init()\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU unavailable: Using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed Python random seed\n",
        "random.seed(233)\n",
        "\n",
        "# Fixed NumPy random seed\n",
        "np.random.seed(233)\n",
        "\n",
        "# Fixed PyTorch random seed\n",
        "torch.manual_seed(233)\n",
        "\n",
        "# Fixed CUDA randomness\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(233)\n",
        "    torch.cuda.manual_seed_all(233)  # 多GPU的情况\n",
        "    torch.backends.cudnn.deterministic = True  # 确保 cudnn 计算是确定性的\n",
        "    torch.backends.cudnn.benchmark = False  # 可能会影响性能，但保证可复现性"
      ],
      "metadata": {
        "id": "Eav3SciVUYyN"
      },
      "id": "Eav3SciVUYyN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Sales class for handling sales data and choice probability calculation\n",
        "class Sales:\n",
        "    def __init__(self, all_offer_sets, sell_num, mask):\n",
        "        # all_offer_sets: list [offerset1: (num_products, num_features), offerset2: (num_products, num_features)...]\n",
        "        # sell_num: nd.array (num_offers, num_products)\n",
        "        # get the number of offers in a sales dataset\n",
        "        self.offer_set_list = []\n",
        "        self.fw = None\n",
        "        self.N_sales = torch.tensor(np.concatenate(sell_num, axis=1), dtype=torch.float64, device=device)\n",
        "        self.offer_feature = torch.tensor(all_offer_sets, dtype=torch.float64)\n",
        "        self.original_shape = self.offer_feature.shape\n",
        "        self.feature_concat = self.offer_feature.reshape(-1, self.offer_feature.shape[2]).to(device)\n",
        "        self.N = torch.sum(self.N_sales)\n",
        "        self.mask = torch.tensor(mask, dtype=torch.float64, device=device)\n",
        "        self.mask_flat = self.mask.reshape((-1,))\n",
        "        self.masked_feature_concat = self.feature_concat[self.mask_flat == 1]\n",
        "\n",
        "    def calculate_all_choice_prob(self, W):\n",
        "        # W: rule-weight taste vector (|Ruleset|, 1)\n",
        "        # ruleset: Rule object\n",
        "        rule_feature = self.feature_concat\n",
        "        self.fw = self.calculate_choice_prob(rule_feature, W)\n",
        "        return self.fw\n",
        "\n",
        "    def calculate_choice_prob(self, rule_feature, W):\n",
        "        \"\"\"output choice probability tensor for every product\"\"\"\n",
        "        # W: rule-weight taste vector (|Ruleset|, 1)\n",
        "        # Z:  (｜St｜, |Ruleset|) * (|Ruleset|, 1) -> (｜St｜, 1)\n",
        "        Z = torch.matmul(rule_feature, W).reshape(self.original_shape[:2])\n",
        "        masked_Z = Z.masked_fill(self.mask == 0, float('-inf'))\n",
        "        softmax_result = F.softmax(masked_Z, dim=-1).reshape((-1, 1))[self.mask_flat == 1]\n",
        "        return softmax_result"
      ],
      "metadata": {
        "id": "MJING83iN1Gj"
      },
      "id": "MJING83iN1Gj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Problem_FW class to handle optimization using the Frank-Wolfe algorithm\n",
        "class Problem_FW:\n",
        "    def __init__(self, S, N, M):\n",
        "        # S: [St] the list of offerset N: [Njt] the sales of jth product in St offer set\n",
        "        self.alpha = torch.ones(1, dtype=torch.float64, device=device)\n",
        "        self.soft_RMSE = None\n",
        "        self.hard_RMSE = None\n",
        "        self.fw_estimate = None\n",
        "        self.sales = Sales(S, N, M)\n",
        "        # Define and initialize the ruleset\n",
        "        self.feature_num = S.shape[-1]\n",
        "        # Put the feature to the GPU\n",
        "        self.sales.feature_concat = self.sales.feature_concat.to(device)\n",
        "        # Define a consumer list that contain consumer types\n",
        "        self.consumer_list = []\n",
        "        # Define the main problem NLL loss\n",
        "        self.NLL_main = None\n",
        "        # Define the current likelihood convex combination\n",
        "        self.g = None\n",
        "        # Define the current likelihood gradient for support finding\n",
        "        self.NLL_gradient = None\n",
        "        # Define a list to contain all fw choice likelihood\n",
        "        self.fw_list = []\n",
        "        # Define a new sales data for further estimation\n",
        "        self.sales_estimate = None\n",
        "\n",
        "    def initialize(self):\n",
        "        # 1. We initialize a taste vector W\n",
        "        W = torch.empty((self.feature_num, 1), dtype=torch.float64, device=device, requires_grad=True)\n",
        "        a = 0\n",
        "        b = 0\n",
        "        init.uniform_(W, a, b)\n",
        "        # 2. Add the initial consumer type in the consumer list\n",
        "        initial_type = ConsumerType(\n",
        "            W,\n",
        "            torch.tensor([1], dtype=torch.float64, requires_grad=True, device=device),\n",
        "            self.sales)\n",
        "        self.consumer_list.append(initial_type)\n",
        "        self.fw_list.append(initial_type.fw)\n",
        "        # 4. Update the main problem NLL loss\n",
        "        self.main_problem_loss()\n",
        "\n",
        "    def main_problem_loss(self):\n",
        "        \"\"\"Here we calculate the NLL Loss for the main problem\"\"\"\n",
        "        N = self.sales.N\n",
        "        normalize_term = torch.tensor(1 / N, dtype=torch.float64, device=device)\n",
        "        N_sales = self.sales.N_sales\n",
        "        # Now we get the combination of the consumer choice likelihood\n",
        "        # this step can also be done by inner product by setting a tensor in problem to store all consumer data\n",
        "        f = torch.zeros(N_sales.shape, dtype=torch.float64, device=device).t()\n",
        "        for ct in self.consumer_list:\n",
        "            f += ct.alpha * ct.fw\n",
        "        f_log = torch.log(f)\n",
        "        self.NLL_main = -normalize_term * torch.matmul(N_sales, f_log)\n",
        "        # calculate the current g\n",
        "        self.g = f\n",
        "        # Update the gradient for the next support finding step\n",
        "        with torch.no_grad():\n",
        "            self.NLL_gradient = -normalize_term * torch.mul(1 / self.g.t(), N_sales)\n",
        "        return self.NLL_main\n",
        "\n",
        "    def support_finding_loss(self, W):\n",
        "        \"\"\"Here we calculate the Loss for the support finding step\"\"\"\n",
        "        fw = self.sales.calculate_all_choice_prob(W)\n",
        "        fw_log = torch.log(fw)\n",
        "        return torch.matmul(self.NLL_gradient, fw_log)\n",
        "\n",
        "    def proportion_update_loss(self, alpha):\n",
        "        N = self.sales.N\n",
        "        alpha = F.softmax(alpha, dim=0)\n",
        "        normalize_term = torch.tensor(1 / N, dtype=torch.float64, device=device)\n",
        "        N_sales = self.sales.N_sales\n",
        "        with torch.no_grad():\n",
        "            fw_tensor = torch.cat(self.fw_list, dim=1)\n",
        "        f = torch.matmul(fw_tensor, alpha)\n",
        "        f_log = torch.log(f)\n",
        "        return -normalize_term * torch.matmul(N_sales, f_log)\n",
        "\n",
        "    def search_for_next_consumer_type(self):\n",
        "        print('-----Consumer Type Search Begin-----')\n",
        "        # Initialize the taste vector w\n",
        "        W = torch.empty((self.feature_num, 1), dtype=torch.float64, device=device, requires_grad=True)\n",
        "        a = -5e-1\n",
        "        b = 5e-1\n",
        "        init.uniform_(W, a, b)\n",
        "        # The loss in the support find step i.e. the sub-problem in conditional gradient descent\n",
        "        print('-----Rule Weight Optimization-----')\n",
        "        loss_previous = 1e100\n",
        "        optimizer = optim.Adam([W], lr=3e-4)\n",
        "        optimizer.zero_grad()\n",
        "        w_previous = None\n",
        "        for epoch in range(2000):\n",
        "            loss = self.support_finding_loss(W)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f'Epoch [{epoch + 1}/{2000}], Loss: {loss.item():.4f}')\n",
        "            loss_current = loss.item()\n",
        "            if loss_current < loss_previous:\n",
        "                loss_previous = loss_current\n",
        "                with torch.no_grad():\n",
        "                    w_previous = W\n",
        "            else:\n",
        "                W = w_previous.requires_grad_(True)\n",
        "                break\n",
        "\n",
        "        new_consumer = ConsumerType(\n",
        "            W,\n",
        "            torch.tensor([0], dtype=torch.float64, device=device),\n",
        "            self.sales)\n",
        "        self.consumer_list.append(new_consumer)\n",
        "        self.fw_list.append(new_consumer.fw)\n",
        "        print('-----Consumer Type Search End-----')\n",
        "\n",
        "    def proportion_update(self):\n",
        "        print('-----Proportion Update Search Begin-----')\n",
        "        # new_alpha = torch.zeros(1, requires_grad=True, dtype=torch.float64, device=device)\n",
        "        # with torch.no_grad():\n",
        "        #     alpha = torch.concatenate((self.alpha, new_alpha))\n",
        "        # alpha = alpha.requires_grad_(True)\n",
        "        alpha = torch.empty((len(self.consumer_list), 1), dtype=torch.float64, device=device, requires_grad=True)\n",
        "        a = 0\n",
        "        b = 0\n",
        "        init.uniform_(alpha, a, b)\n",
        "        optimizer = optim.Adam([alpha], lr=5e-3)\n",
        "        optimizer.zero_grad()\n",
        "        loss_previous = 1e100\n",
        "        alpha_previous = None\n",
        "        for epoch in range(1000):\n",
        "            loss = self.proportion_update_loss(alpha)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f'Epoch [{epoch + 1}/{50000}], Loss: {loss.item():.4f}')\n",
        "            loss_current = loss.item()\n",
        "            if loss_current < loss_previous:\n",
        "                loss_previous = loss_current\n",
        "                with torch.no_grad():\n",
        "                    alpha_previous = alpha\n",
        "            else:\n",
        "                alpha = alpha_previous.requires_grad_(True)\n",
        "                break\n",
        "        self.alpha = alpha\n",
        "        alpha = F.softmax(alpha, dim=0)\n",
        "        for i, consumer in enumerate(self.consumer_list):\n",
        "            consumer.alpha = alpha[i]\n",
        "        self.main_problem_loss()\n",
        "        print('-----Proportion Update Search End-----')"
      ],
      "metadata": {
        "id": "_Eq2rMGNMe7O"
      },
      "id": "_Eq2rMGNMe7O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def estimate_NLL(self, sales_for_estimate, N_estimate, mask_for_estimate):\n",
        "            self.sales_estimate = Sales(sales_for_estimate, N_estimate, mask_for_estimate)\n",
        "            N = self.sales_estimate.N\n",
        "            normalize_term = torch.tensor(1 / N, dtype=torch.float64, device=device)\n",
        "            N_sales = self.sales_estimate.N_sales\n",
        "            f = torch.zeros(N_sales.shape, dtype=torch.float64, device=device).t()\n",
        "            for ct in self.consumer_list:\n",
        "                f += ct.alpha * self.sales_estimate.calculate_all_choice_prob(ct.taste)\n",
        "            f_log = torch.log(f)\n",
        "            return -normalize_term * torch.matmul(N_sales, f_log), f\n",
        "\n",
        "    def estimate_rank(self, sales_for_estimate, N_estimate, mask_for_estimate):\n",
        "        rank_list = []\n",
        "        for o, s, msk in zip(sales_for_estimate, N_estimate, mask_for_estimate):\n",
        "            prediction = self.estimate_NLL(np.array([o]),\n",
        "                                           np.array([s]),\n",
        "                                           np.array([msk]))[1].flatten().detach().cpu().numpy()\n",
        "            choice = np.where(s[0] == 1)[0][0]\n",
        "            value_at_index = prediction[choice]\n",
        "            sorted_prediction = np.sort(prediction)\n",
        "            rank = np.searchsorted(sorted_prediction, value_at_index, side='right')\n",
        "            percentile_rank = (1 - (rank / len(s[0]))) * 100\n",
        "            rank_list.append(percentile_rank)\n",
        "        return rank_list\n"
      ],
      "metadata": {
        "id": "1fdZmwMgPo2z"
      },
      "id": "1fdZmwMgPo2z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConsumerType:\n",
        "    def __init__(self, weight, alpha, problem_sales):\n",
        "        # rule_weight (tensor): the weight of each rule in the consideration rules (|consideration_rule|, 1)\n",
        "        self.taste = weight\n",
        "        # alpha (float.64): the proportion of this type in the distribution\n",
        "        self.alpha = alpha\n",
        "        # fw: the choice likelihood vector for the consumer type\n",
        "        # problem_sales: the sales object for this problem\n",
        "        self.fw = problem_sales.calculate_all_choice_prob(self.taste)"
      ],
      "metadata": {
        "id": "NY8j4Lx7Nplo"
      },
      "id": "NY8j4Lx7Nplo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_frank_wolfe(type_num, tr_offerset_list, tr_sell_list, tr_mask_list):\n",
        "    problem = Problem_FW(tr_offerset_list, tr_sell_list, tr_mask_list)\n",
        "    problem.initialize()\n",
        "    n = type_num\n",
        "    NLL_LOSS_LIST = [problem.NLL_main.item()]\n",
        "    for m in range(n):\n",
        "        print('-----Consumer Type ' + str(m + 1) + ' Start Searching------')\n",
        "        problem.search_for_next_consumer_type()\n",
        "        problem.proportion_update()\n",
        "        print('main problem loss: ', problem.NLL_main.item())\n",
        "        print('-----One Iteration Done-----')\n",
        "        NLL_LOSS_LIST.append(problem.NLL_main.item())\n",
        "    for i in range(1, n + 1):\n",
        "        print('-----Consumer Type ' + str(i) + '------')\n",
        "        print('Consumer Proportion:', problem.consumer_list[i].alpha.item())\n",
        "    return problem, NLL_LOSS_LIST\n",
        "\n",
        "\n",
        "def get_offer_data(data_para):\n",
        "    offerset_list = []\n",
        "    sell_list = []\n",
        "    mask_list = []\n",
        "    max_num = 32\n",
        "    for srch_id, group in data_para:\n",
        "        num_product = len(group)\n",
        "        offerset = group.drop(columns=['booking_bool', 'srch_id']).values\n",
        "        offer_dummy = np.zeros((max_num - num_product, offerset.shape[1]))\n",
        "        offerset = np.vstack((offerset, offer_dummy))\n",
        "        offer_mask = np.append(np.ones(num_product), np.zeros(max_num - num_product))\n",
        "        num_sell = group['booking_bool'].values.reshape(1, -1)\n",
        "        offerset_list.append(offerset)\n",
        "        sell_list.append(num_sell)\n",
        "        mask_list.append(offer_mask)\n",
        "\n",
        "    offerset_list = np.array(offerset_list)\n",
        "    mask_list = np.array(mask_list)\n",
        "    return offerset_list, sell_list, mask_list"
      ],
      "metadata": {
        "id": "svDzW38nNj3l"
      },
      "id": "svDzW38nNj3l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training and testing data from CSV files\n",
        "search_info = ['srch_id']\n",
        "feature = ['position', 'prop_starrating',\n",
        "           'prop_location_score1', 'prop_log_historical_price',\n",
        "           'prop_brand_bool', 'promotion_flag', 'srch_booking_window', 'srch_length_of_stay',\n",
        "           'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
        "           'srch_saturday_night_bool', 'price_usd', 'random_bool', 'booking_bool']\n",
        "\n",
        "tr_data = pd.read_csv('/content/drive/MyDrive/STAT4830 Group Project/Week4/train_28-32_10000.csv')\n",
        "te_data = pd.read_csv('/content/drive/MyDrive/STAT4830 Group Project/Week4/test_28-32_1000.csv')\n",
        "tr_data = tr_data[search_info + feature]\n",
        "te_data = te_data[search_info + feature]\n",
        "tr_data = tr_data.sort_values(by=['srch_id'])\n",
        "te_data = te_data.sort_values(by=['srch_id'])\n",
        "tr_offerset_list, tr_sell_list, tr_mask_list = get_offer_data(tr_data.groupby('srch_id'))\n",
        "te_offerset_list, te_sell_list, te_mask_list = get_offer_data(te_data.groupby('srch_id'))"
      ],
      "metadata": {
        "id": "r-tx5zawNect"
      },
      "id": "r-tx5zawNect",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNL_problem, NLL_LIST = train_frank_wolfe(20, tr_offerset_list, tr_sell_list, tr_mask_list)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(NLL_LIST, marker='o', linestyle='-', color='b', label='FW')\n",
        "plt.title('NLL LOSS VALUE')\n",
        "plt.xlabel('ITERATION')\n",
        "plt.ylabel('NLL LOSS')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF9lme-3OMme",
        "outputId": "c54f8d38-892a-4244-8e50-64f09bf0d194"
      },
      "id": "vF9lme-3OMme",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-b3df75118721>:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  normalize_term = torch.tensor(1 / N, dtype=torch.float64, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Consumer Type 1 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 1461.3168\n",
            "Epoch [20/2000], Loss: 1440.5429\n",
            "Epoch [30/2000], Loss: 1417.5207\n",
            "Epoch [40/2000], Loss: 1392.6592\n",
            "Epoch [50/2000], Loss: 1366.3896\n",
            "Epoch [60/2000], Loss: 1339.0713\n",
            "Epoch [70/2000], Loss: 1310.9762\n",
            "Epoch [80/2000], Loss: 1282.3019\n",
            "Epoch [90/2000], Loss: 1253.1906\n",
            "Epoch [100/2000], Loss: 1223.7454\n",
            "Epoch [110/2000], Loss: 1194.0426\n",
            "Epoch [120/2000], Loss: 1164.1399\n",
            "Epoch [130/2000], Loss: 1134.0820\n",
            "Epoch [140/2000], Loss: 1103.9044\n",
            "Epoch [150/2000], Loss: 1073.6358\n",
            "Epoch [160/2000], Loss: 1043.3002\n",
            "Epoch [170/2000], Loss: 1012.9179\n",
            "Epoch [180/2000], Loss: 982.5065\n",
            "Epoch [190/2000], Loss: 952.0813\n",
            "Epoch [200/2000], Loss: 921.6565\n",
            "Epoch [210/2000], Loss: 891.2450\n",
            "Epoch [220/2000], Loss: 860.8589\n",
            "Epoch [230/2000], Loss: 830.5099\n",
            "Epoch [240/2000], Loss: 800.2095\n",
            "Epoch [250/2000], Loss: 769.9692\n",
            "Epoch [260/2000], Loss: 739.8008\n",
            "Epoch [270/2000], Loss: 709.7164\n",
            "Epoch [280/2000], Loss: 679.7292\n",
            "Epoch [290/2000], Loss: 649.8534\n",
            "Epoch [300/2000], Loss: 620.1045\n",
            "Epoch [310/2000], Loss: 590.5005\n",
            "Epoch [320/2000], Loss: 561.0615\n",
            "Epoch [330/2000], Loss: 531.8111\n",
            "Epoch [340/2000], Loss: 502.7763\n",
            "Epoch [350/2000], Loss: 473.9889\n",
            "Epoch [360/2000], Loss: 445.4859\n",
            "Epoch [370/2000], Loss: 417.3108\n",
            "Epoch [380/2000], Loss: 389.5154\n",
            "Epoch [390/2000], Loss: 362.1610\n",
            "Epoch [400/2000], Loss: 335.3216\n",
            "Epoch [410/2000], Loss: 309.0881\n",
            "Epoch [420/2000], Loss: 283.5756\n",
            "Epoch [430/2000], Loss: 258.9315\n",
            "Epoch [440/2000], Loss: 235.3430\n",
            "Epoch [450/2000], Loss: 213.0459\n",
            "Epoch [460/2000], Loss: 192.3266\n",
            "Epoch [470/2000], Loss: 173.5164\n",
            "Epoch [480/2000], Loss: 156.9907\n",
            "Epoch [490/2000], Loss: 143.1457\n",
            "Epoch [500/2000], Loss: 132.3408\n",
            "Epoch [510/2000], Loss: 124.6855\n",
            "Epoch [520/2000], Loss: 119.9112\n",
            "Epoch [530/2000], Loss: 117.6037\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0242\n",
            "Epoch [20/50000], Loss: 3.0234\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  3.0234091720296603\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 2 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 609.0617\n",
            "Epoch [20/2000], Loss: 601.9515\n",
            "Epoch [30/2000], Loss: 594.0723\n",
            "Epoch [40/2000], Loss: 585.5645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-b3df75118721>:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  normalize_term = torch.tensor(1 / N, dtype=torch.float64, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/2000], Loss: 576.5756\n",
            "Epoch [60/2000], Loss: 567.2289\n",
            "Epoch [70/2000], Loss: 557.6176\n",
            "Epoch [80/2000], Loss: 547.8094\n",
            "Epoch [90/2000], Loss: 537.8530\n",
            "Epoch [100/2000], Loss: 527.7839\n",
            "Epoch [110/2000], Loss: 517.6283\n",
            "Epoch [120/2000], Loss: 507.4059\n",
            "Epoch [130/2000], Loss: 497.1321\n",
            "Epoch [140/2000], Loss: 486.8192\n",
            "Epoch [150/2000], Loss: 476.4770\n",
            "Epoch [160/2000], Loss: 466.1137\n",
            "Epoch [170/2000], Loss: 455.7363\n",
            "Epoch [180/2000], Loss: 445.3509\n",
            "Epoch [190/2000], Loss: 434.9627\n",
            "Epoch [200/2000], Loss: 424.5765\n",
            "Epoch [210/2000], Loss: 414.1967\n",
            "Epoch [220/2000], Loss: 403.8273\n",
            "Epoch [230/2000], Loss: 393.4723\n",
            "Epoch [240/2000], Loss: 383.1354\n",
            "Epoch [250/2000], Loss: 372.8202\n",
            "Epoch [260/2000], Loss: 362.5305\n",
            "Epoch [270/2000], Loss: 352.2701\n",
            "Epoch [280/2000], Loss: 342.0426\n",
            "Epoch [290/2000], Loss: 331.8523\n",
            "Epoch [300/2000], Loss: 321.7032\n",
            "Epoch [310/2000], Loss: 311.5999\n",
            "Epoch [320/2000], Loss: 301.5471\n",
            "Epoch [330/2000], Loss: 291.5501\n",
            "Epoch [340/2000], Loss: 281.6145\n",
            "Epoch [350/2000], Loss: 271.7466\n",
            "Epoch [360/2000], Loss: 261.9531\n",
            "Epoch [370/2000], Loss: 252.2417\n",
            "Epoch [380/2000], Loss: 242.6210\n",
            "Epoch [390/2000], Loss: 233.1003\n",
            "Epoch [400/2000], Loss: 223.6904\n",
            "Epoch [410/2000], Loss: 214.4035\n",
            "Epoch [420/2000], Loss: 205.2534\n",
            "Epoch [430/2000], Loss: 196.2558\n",
            "Epoch [440/2000], Loss: 187.4289\n",
            "Epoch [450/2000], Loss: 178.7935\n",
            "Epoch [460/2000], Loss: 170.3740\n",
            "Epoch [470/2000], Loss: 162.1986\n",
            "Epoch [480/2000], Loss: 154.3002\n",
            "Epoch [490/2000], Loss: 146.7179\n",
            "Epoch [500/2000], Loss: 139.4976\n",
            "Epoch [510/2000], Loss: 132.6939\n",
            "Epoch [520/2000], Loss: 126.3723\n",
            "Epoch [530/2000], Loss: 120.6117\n",
            "Epoch [540/2000], Loss: 115.5081\n",
            "Epoch [550/2000], Loss: 111.1797\n",
            "Epoch [560/2000], Loss: 107.7753\n",
            "Epoch [570/2000], Loss: 105.4893\n",
            "Epoch [580/2000], Loss: 104.5998\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0312\n",
            "Epoch [20/50000], Loss: 3.0237\n",
            "Epoch [30/50000], Loss: 3.0165\n",
            "Epoch [40/50000], Loss: 3.0105\n",
            "Epoch [50/50000], Loss: 3.0060\n",
            "Epoch [60/50000], Loss: 3.0035\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  3.002933126449516\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 3 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 495.1902\n",
            "Epoch [20/2000], Loss: 488.0191\n",
            "Epoch [30/2000], Loss: 480.0768\n",
            "Epoch [40/2000], Loss: 471.5059\n",
            "Epoch [50/2000], Loss: 462.4570\n",
            "Epoch [60/2000], Loss: 453.0555\n",
            "Epoch [70/2000], Loss: 443.3964\n",
            "Epoch [80/2000], Loss: 433.5491\n",
            "Epoch [90/2000], Loss: 423.5640\n",
            "Epoch [100/2000], Loss: 413.4778\n",
            "Epoch [110/2000], Loss: 403.3180\n",
            "Epoch [120/2000], Loss: 393.1060\n",
            "Epoch [130/2000], Loss: 382.8585\n",
            "Epoch [140/2000], Loss: 372.5893\n",
            "Epoch [150/2000], Loss: 362.3100\n",
            "Epoch [160/2000], Loss: 352.0305\n",
            "Epoch [170/2000], Loss: 341.7598\n",
            "Epoch [180/2000], Loss: 331.5061\n",
            "Epoch [190/2000], Loss: 321.2771\n",
            "Epoch [200/2000], Loss: 311.0800\n",
            "Epoch [210/2000], Loss: 300.9222\n",
            "Epoch [220/2000], Loss: 290.8110\n",
            "Epoch [230/2000], Loss: 280.7539\n",
            "Epoch [240/2000], Loss: 270.7587\n",
            "Epoch [250/2000], Loss: 260.8335\n",
            "Epoch [260/2000], Loss: 250.9873\n",
            "Epoch [270/2000], Loss: 241.2295\n",
            "Epoch [280/2000], Loss: 231.5706\n",
            "Epoch [290/2000], Loss: 222.0221\n",
            "Epoch [300/2000], Loss: 212.5970\n",
            "Epoch [310/2000], Loss: 203.3097\n",
            "Epoch [320/2000], Loss: 194.1766\n",
            "Epoch [330/2000], Loss: 185.2162\n",
            "Epoch [340/2000], Loss: 176.4502\n",
            "Epoch [350/2000], Loss: 167.9031\n",
            "Epoch [360/2000], Loss: 159.6039\n",
            "Epoch [370/2000], Loss: 151.5863\n",
            "Epoch [380/2000], Loss: 143.8898\n",
            "Epoch [390/2000], Loss: 136.5617\n",
            "Epoch [400/2000], Loss: 129.6579\n",
            "Epoch [410/2000], Loss: 123.2459\n",
            "Epoch [420/2000], Loss: 117.4069\n",
            "Epoch [430/2000], Loss: 112.2405\n",
            "Epoch [440/2000], Loss: 107.8697\n",
            "Epoch [450/2000], Loss: 104.4519\n",
            "Epoch [460/2000], Loss: 102.1978\n",
            "Epoch [470/2000], Loss: 101.4179\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0387\n",
            "Epoch [20/50000], Loss: 3.0293\n",
            "Epoch [30/50000], Loss: 3.0196\n",
            "Epoch [40/50000], Loss: 3.0104\n",
            "Epoch [50/50000], Loss: 3.0020\n",
            "Epoch [60/50000], Loss: 2.9951\n",
            "Epoch [70/50000], Loss: 2.9901\n",
            "Epoch [80/50000], Loss: 2.9870\n",
            "Epoch [90/50000], Loss: 2.9861\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9861427974819152\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 4 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 977.5407\n",
            "Epoch [20/2000], Loss: 969.8304\n",
            "Epoch [30/2000], Loss: 961.2910\n",
            "Epoch [40/2000], Loss: 952.0759\n",
            "Epoch [50/2000], Loss: 942.3465\n",
            "Epoch [60/2000], Loss: 932.2370\n",
            "Epoch [70/2000], Loss: 921.8492\n",
            "Epoch [80/2000], Loss: 911.2569\n",
            "Epoch [90/2000], Loss: 900.5132\n",
            "Epoch [100/2000], Loss: 889.6567\n",
            "Epoch [110/2000], Loss: 878.7159\n",
            "Epoch [120/2000], Loss: 867.7122\n",
            "Epoch [130/2000], Loss: 856.6623\n",
            "Epoch [140/2000], Loss: 845.5792\n",
            "Epoch [150/2000], Loss: 834.4735\n",
            "Epoch [160/2000], Loss: 823.3539\n",
            "Epoch [170/2000], Loss: 812.2275\n",
            "Epoch [180/2000], Loss: 801.1006\n",
            "Epoch [190/2000], Loss: 789.9782\n",
            "Epoch [200/2000], Loss: 778.8651\n",
            "Epoch [210/2000], Loss: 767.7651\n",
            "Epoch [220/2000], Loss: 756.6819\n",
            "Epoch [230/2000], Loss: 745.6186\n",
            "Epoch [240/2000], Loss: 734.5782\n",
            "Epoch [250/2000], Loss: 723.5632\n",
            "Epoch [260/2000], Loss: 712.5762\n",
            "Epoch [270/2000], Loss: 701.6194\n",
            "Epoch [280/2000], Loss: 690.6948\n",
            "Epoch [290/2000], Loss: 679.8045\n",
            "Epoch [300/2000], Loss: 668.9504\n",
            "Epoch [310/2000], Loss: 658.1341\n",
            "Epoch [320/2000], Loss: 647.3575\n",
            "Epoch [330/2000], Loss: 636.6221\n",
            "Epoch [340/2000], Loss: 625.9296\n",
            "Epoch [350/2000], Loss: 615.2815\n",
            "Epoch [360/2000], Loss: 604.6794\n",
            "Epoch [370/2000], Loss: 594.1249\n",
            "Epoch [380/2000], Loss: 583.6195\n",
            "Epoch [390/2000], Loss: 573.1648\n",
            "Epoch [400/2000], Loss: 562.7622\n",
            "Epoch [410/2000], Loss: 552.4133\n",
            "Epoch [420/2000], Loss: 542.1197\n",
            "Epoch [430/2000], Loss: 531.8827\n",
            "Epoch [440/2000], Loss: 521.7039\n",
            "Epoch [450/2000], Loss: 511.5846\n",
            "Epoch [460/2000], Loss: 501.5263\n",
            "Epoch [470/2000], Loss: 491.5302\n",
            "Epoch [480/2000], Loss: 481.5976\n",
            "Epoch [490/2000], Loss: 471.7297\n",
            "Epoch [500/2000], Loss: 461.9278\n",
            "Epoch [510/2000], Loss: 452.1927\n",
            "Epoch [520/2000], Loss: 442.5256\n",
            "Epoch [530/2000], Loss: 432.9273\n",
            "Epoch [540/2000], Loss: 423.3986\n",
            "Epoch [550/2000], Loss: 413.9402\n",
            "Epoch [560/2000], Loss: 404.5526\n",
            "Epoch [570/2000], Loss: 395.2363\n",
            "Epoch [580/2000], Loss: 385.9916\n",
            "Epoch [590/2000], Loss: 376.8187\n",
            "Epoch [600/2000], Loss: 367.7176\n",
            "Epoch [610/2000], Loss: 358.6881\n",
            "Epoch [620/2000], Loss: 349.7300\n",
            "Epoch [630/2000], Loss: 340.8430\n",
            "Epoch [640/2000], Loss: 332.0266\n",
            "Epoch [650/2000], Loss: 323.2804\n",
            "Epoch [660/2000], Loss: 314.6037\n",
            "Epoch [670/2000], Loss: 305.9958\n",
            "Epoch [680/2000], Loss: 297.4561\n",
            "Epoch [690/2000], Loss: 288.9840\n",
            "Epoch [700/2000], Loss: 280.5788\n",
            "Epoch [710/2000], Loss: 272.2401\n",
            "Epoch [720/2000], Loss: 263.9677\n",
            "Epoch [730/2000], Loss: 255.7620\n",
            "Epoch [740/2000], Loss: 247.6236\n",
            "Epoch [750/2000], Loss: 239.5542\n",
            "Epoch [760/2000], Loss: 231.5564\n",
            "Epoch [770/2000], Loss: 223.6337\n",
            "Epoch [780/2000], Loss: 215.7916\n",
            "Epoch [790/2000], Loss: 208.0373\n",
            "Epoch [800/2000], Loss: 200.3807\n",
            "Epoch [810/2000], Loss: 192.8347\n",
            "Epoch [820/2000], Loss: 185.4160\n",
            "Epoch [830/2000], Loss: 178.1457\n",
            "Epoch [840/2000], Loss: 171.0501\n",
            "Epoch [850/2000], Loss: 164.1612\n",
            "Epoch [860/2000], Loss: 157.5172\n",
            "Epoch [870/2000], Loss: 151.1637\n",
            "Epoch [880/2000], Loss: 145.1539\n",
            "Epoch [890/2000], Loss: 139.5495\n",
            "Epoch [900/2000], Loss: 134.4230\n",
            "Epoch [910/2000], Loss: 129.8612\n",
            "Epoch [920/2000], Loss: 125.9682\n",
            "Epoch [930/2000], Loss: 122.8704\n",
            "Epoch [940/2000], Loss: 120.7290\n",
            "Epoch [950/2000], Loss: 119.7640\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 2.9772\n",
            "Epoch [20/50000], Loss: 2.9690\n",
            "Epoch [30/50000], Loss: 2.9610\n",
            "Epoch [40/50000], Loss: 2.9537\n",
            "Epoch [50/50000], Loss: 2.9476\n",
            "Epoch [60/50000], Loss: 2.9430\n",
            "Epoch [70/50000], Loss: 2.9400\n",
            "Epoch [80/50000], Loss: 2.9386\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.938568820884096\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 5 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 139.3893\n",
            "Epoch [20/2000], Loss: 134.5394\n",
            "Epoch [30/2000], Loss: 129.5122\n",
            "Epoch [40/2000], Loss: 124.5661\n",
            "Epoch [50/2000], Loss: 119.9866\n",
            "Epoch [60/2000], Loss: 116.0714\n",
            "Epoch [70/2000], Loss: 113.1455\n",
            "Epoch [80/2000], Loss: 111.6050\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0490\n",
            "Epoch [20/50000], Loss: 3.0353\n",
            "Epoch [30/50000], Loss: 3.0211\n",
            "Epoch [40/50000], Loss: 3.0071\n",
            "Epoch [50/50000], Loss: 2.9939\n",
            "Epoch [60/50000], Loss: 2.9819\n",
            "Epoch [70/50000], Loss: 2.9717\n",
            "Epoch [80/50000], Loss: 2.9634\n",
            "Epoch [90/50000], Loss: 2.9571\n",
            "Epoch [100/50000], Loss: 2.9528\n",
            "Epoch [110/50000], Loss: 2.9504\n",
            "Epoch [120/50000], Loss: 2.9497\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9497219087801847\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 6 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 665.8120\n",
            "Epoch [20/2000], Loss: 658.6945\n",
            "Epoch [30/2000], Loss: 650.8067\n",
            "Epoch [40/2000], Loss: 642.2885\n",
            "Epoch [50/2000], Loss: 633.2878\n",
            "Epoch [60/2000], Loss: 623.9276\n",
            "Epoch [70/2000], Loss: 614.3009\n",
            "Epoch [80/2000], Loss: 604.4755\n",
            "Epoch [90/2000], Loss: 594.5000\n",
            "Epoch [100/2000], Loss: 584.4095\n",
            "Epoch [110/2000], Loss: 574.2302\n",
            "Epoch [120/2000], Loss: 563.9817\n",
            "Epoch [130/2000], Loss: 553.6791\n",
            "Epoch [140/2000], Loss: 543.3344\n",
            "Epoch [150/2000], Loss: 532.9572\n",
            "Epoch [160/2000], Loss: 522.5555\n",
            "Epoch [170/2000], Loss: 512.1360\n",
            "Epoch [180/2000], Loss: 501.7043\n",
            "Epoch [190/2000], Loss: 491.2653\n",
            "Epoch [200/2000], Loss: 480.8235\n",
            "Epoch [210/2000], Loss: 470.3827\n",
            "Epoch [220/2000], Loss: 459.9464\n",
            "Epoch [230/2000], Loss: 449.5181\n",
            "Epoch [240/2000], Loss: 439.1008\n",
            "Epoch [250/2000], Loss: 428.6974\n",
            "Epoch [260/2000], Loss: 418.3109\n",
            "Epoch [270/2000], Loss: 407.9441\n",
            "Epoch [280/2000], Loss: 397.5999\n",
            "Epoch [290/2000], Loss: 387.2810\n",
            "Epoch [300/2000], Loss: 376.9905\n",
            "Epoch [310/2000], Loss: 366.7313\n",
            "Epoch [320/2000], Loss: 356.5067\n",
            "Epoch [330/2000], Loss: 346.3198\n",
            "Epoch [340/2000], Loss: 336.1744\n",
            "Epoch [350/2000], Loss: 326.0742\n",
            "Epoch [360/2000], Loss: 316.0231\n",
            "Epoch [370/2000], Loss: 306.0258\n",
            "Epoch [380/2000], Loss: 296.0869\n",
            "Epoch [390/2000], Loss: 286.2117\n",
            "Epoch [400/2000], Loss: 276.4061\n",
            "Epoch [410/2000], Loss: 266.6764\n",
            "Epoch [420/2000], Loss: 257.0298\n",
            "Epoch [430/2000], Loss: 247.4742\n",
            "Epoch [440/2000], Loss: 238.0184\n",
            "Epoch [450/2000], Loss: 228.6725\n",
            "Epoch [460/2000], Loss: 219.4477\n",
            "Epoch [470/2000], Loss: 210.3567\n",
            "Epoch [480/2000], Loss: 201.4139\n",
            "Epoch [490/2000], Loss: 192.6360\n",
            "Epoch [500/2000], Loss: 184.0417\n",
            "Epoch [510/2000], Loss: 175.6531\n",
            "Epoch [520/2000], Loss: 167.4952\n",
            "Epoch [530/2000], Loss: 159.5976\n",
            "Epoch [540/2000], Loss: 151.9946\n",
            "Epoch [550/2000], Loss: 144.7268\n",
            "Epoch [560/2000], Loss: 137.8422\n",
            "Epoch [570/2000], Loss: 131.3979\n",
            "Epoch [580/2000], Loss: 125.4628\n",
            "Epoch [590/2000], Loss: 120.1203\n",
            "Epoch [600/2000], Loss: 115.4728\n",
            "Epoch [610/2000], Loss: 111.6476\n",
            "Epoch [620/2000], Loss: 108.8078\n",
            "Epoch [630/2000], Loss: 107.1782\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0767\n",
            "Epoch [20/50000], Loss: 3.0618\n",
            "Epoch [30/50000], Loss: 3.0459\n",
            "Epoch [40/50000], Loss: 3.0297\n",
            "Epoch [50/50000], Loss: 3.0140\n",
            "Epoch [60/50000], Loss: 2.9992\n",
            "Epoch [70/50000], Loss: 2.9860\n",
            "Epoch [80/50000], Loss: 2.9745\n",
            "Epoch [90/50000], Loss: 2.9651\n",
            "Epoch [100/50000], Loss: 2.9577\n",
            "Epoch [110/50000], Loss: 2.9525\n",
            "Epoch [120/50000], Loss: 2.9493\n",
            "Epoch [130/50000], Loss: 2.9479\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9478066980908446\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 7 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 472.7897\n",
            "Epoch [20/2000], Loss: 465.7885\n",
            "Epoch [30/2000], Loss: 458.0364\n",
            "Epoch [40/2000], Loss: 449.6736\n",
            "Epoch [50/2000], Loss: 440.8478\n",
            "Epoch [60/2000], Loss: 431.6818\n",
            "Epoch [70/2000], Loss: 422.2690\n",
            "Epoch [80/2000], Loss: 412.6779\n",
            "Epoch [90/2000], Loss: 402.9578\n",
            "Epoch [100/2000], Loss: 393.1454\n",
            "Epoch [110/2000], Loss: 383.2679\n",
            "Epoch [120/2000], Loss: 373.3469\n",
            "Epoch [130/2000], Loss: 363.3993\n",
            "Epoch [140/2000], Loss: 353.4393\n",
            "Epoch [150/2000], Loss: 343.4790\n",
            "Epoch [160/2000], Loss: 333.5292\n",
            "Epoch [170/2000], Loss: 323.5996\n",
            "Epoch [180/2000], Loss: 313.6995\n",
            "Epoch [190/2000], Loss: 303.8376\n",
            "Epoch [200/2000], Loss: 294.0229\n",
            "Epoch [210/2000], Loss: 284.2642\n",
            "Epoch [220/2000], Loss: 274.5708\n",
            "Epoch [230/2000], Loss: 264.9526\n",
            "Epoch [240/2000], Loss: 255.4198\n",
            "Epoch [250/2000], Loss: 245.9840\n",
            "Epoch [260/2000], Loss: 236.6575\n",
            "Epoch [270/2000], Loss: 227.4542\n",
            "Epoch [280/2000], Loss: 218.3893\n",
            "Epoch [290/2000], Loss: 209.4802\n",
            "Epoch [300/2000], Loss: 200.7465\n",
            "Epoch [310/2000], Loss: 192.2108\n",
            "Epoch [320/2000], Loss: 183.8990\n",
            "Epoch [330/2000], Loss: 175.8414\n",
            "Epoch [340/2000], Loss: 168.0734\n",
            "Epoch [350/2000], Loss: 160.6367\n",
            "Epoch [360/2000], Loss: 153.5805\n",
            "Epoch [370/2000], Loss: 146.9627\n",
            "Epoch [380/2000], Loss: 140.8513\n",
            "Epoch [390/2000], Loss: 135.3258\n",
            "Epoch [400/2000], Loss: 130.4795\n",
            "Epoch [410/2000], Loss: 126.4247\n",
            "Epoch [420/2000], Loss: 123.2998\n",
            "Epoch [430/2000], Loss: 121.2812\n",
            "Epoch [440/2000], Loss: 120.6143\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0385\n",
            "Epoch [20/50000], Loss: 3.0251\n",
            "Epoch [30/50000], Loss: 3.0112\n",
            "Epoch [40/50000], Loss: 2.9975\n",
            "Epoch [50/50000], Loss: 2.9844\n",
            "Epoch [60/50000], Loss: 2.9726\n",
            "Epoch [70/50000], Loss: 2.9624\n",
            "Epoch [80/50000], Loss: 2.9538\n",
            "Epoch [90/50000], Loss: 2.9470\n",
            "Epoch [100/50000], Loss: 2.9419\n",
            "Epoch [110/50000], Loss: 2.9384\n",
            "Epoch [120/50000], Loss: 2.9364\n",
            "Epoch [130/50000], Loss: 2.9356\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9356283786635893\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 8 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 807.7550\n",
            "Epoch [20/2000], Loss: 789.1777\n",
            "Epoch [30/2000], Loss: 768.6259\n",
            "Epoch [40/2000], Loss: 746.4790\n",
            "Epoch [50/2000], Loss: 723.1358\n",
            "Epoch [60/2000], Loss: 698.9309\n",
            "Epoch [70/2000], Loss: 674.1207\n",
            "Epoch [80/2000], Loss: 648.8961\n",
            "Epoch [90/2000], Loss: 623.4003\n",
            "Epoch [100/2000], Loss: 597.7445\n",
            "Epoch [110/2000], Loss: 572.0184\n",
            "Epoch [120/2000], Loss: 546.2994\n",
            "Epoch [130/2000], Loss: 520.6571\n",
            "Epoch [140/2000], Loss: 495.1587\n",
            "Epoch [150/2000], Loss: 469.8729\n",
            "Epoch [160/2000], Loss: 444.8743\n",
            "Epoch [170/2000], Loss: 420.2484\n",
            "Epoch [180/2000], Loss: 396.0971\n",
            "Epoch [190/2000], Loss: 372.5426\n",
            "Epoch [200/2000], Loss: 349.7269\n",
            "Epoch [210/2000], Loss: 327.8090\n",
            "Epoch [220/2000], Loss: 306.9685\n",
            "Epoch [230/2000], Loss: 287.4139\n",
            "Epoch [240/2000], Loss: 269.3853\n",
            "Epoch [250/2000], Loss: 253.1554\n",
            "Epoch [260/2000], Loss: 239.0186\n",
            "Epoch [270/2000], Loss: 227.2099\n",
            "Epoch [280/2000], Loss: 217.8025\n",
            "Epoch [290/2000], Loss: 210.7459\n",
            "Epoch [300/2000], Loss: 205.7895\n",
            "Epoch [310/2000], Loss: 202.4376\n",
            "Epoch [320/2000], Loss: 200.2621\n",
            "Epoch [330/2000], Loss: 198.9892\n",
            "Epoch [340/2000], Loss: 198.4323\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 2.9833\n",
            "Epoch [20/50000], Loss: 2.9727\n",
            "Epoch [30/50000], Loss: 2.9622\n",
            "Epoch [40/50000], Loss: 2.9524\n",
            "Epoch [50/50000], Loss: 2.9438\n",
            "Epoch [60/50000], Loss: 2.9367\n",
            "Epoch [70/50000], Loss: 2.9313\n",
            "Epoch [80/50000], Loss: 2.9275\n",
            "Epoch [90/50000], Loss: 2.9253\n",
            "Epoch [100/50000], Loss: 2.9244\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9243438756675797\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 9 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 2653.4355\n",
            "Epoch [20/2000], Loss: 2633.9051\n",
            "Epoch [30/2000], Loss: 2612.2560\n",
            "Epoch [40/2000], Loss: 2588.8711\n",
            "Epoch [50/2000], Loss: 2564.1541\n",
            "Epoch [60/2000], Loss: 2538.4414\n",
            "Epoch [70/2000], Loss: 2511.9872\n",
            "Epoch [80/2000], Loss: 2484.9760\n",
            "Epoch [90/2000], Loss: 2457.5399\n",
            "Epoch [100/2000], Loss: 2429.7747\n",
            "Epoch [110/2000], Loss: 2401.7504\n",
            "Epoch [120/2000], Loss: 2373.5197\n",
            "Epoch [130/2000], Loss: 2345.1230\n",
            "Epoch [140/2000], Loss: 2316.5915\n",
            "Epoch [150/2000], Loss: 2287.9503\n",
            "Epoch [160/2000], Loss: 2259.2197\n",
            "Epoch [170/2000], Loss: 2230.4160\n",
            "Epoch [180/2000], Loss: 2201.5531\n",
            "Epoch [190/2000], Loss: 2172.6425\n",
            "Epoch [200/2000], Loss: 2143.6940\n",
            "Epoch [210/2000], Loss: 2114.7160\n",
            "Epoch [220/2000], Loss: 2085.7156\n",
            "Epoch [230/2000], Loss: 2056.6993\n",
            "Epoch [240/2000], Loss: 2027.6726\n",
            "Epoch [250/2000], Loss: 1998.6403\n",
            "Epoch [260/2000], Loss: 1969.6069\n",
            "Epoch [270/2000], Loss: 1940.5763\n",
            "Epoch [280/2000], Loss: 1911.5520\n",
            "Epoch [290/2000], Loss: 1882.5372\n",
            "Epoch [300/2000], Loss: 1853.5349\n",
            "Epoch [310/2000], Loss: 1824.5478\n",
            "Epoch [320/2000], Loss: 1795.5783\n",
            "Epoch [330/2000], Loss: 1766.6289\n",
            "Epoch [340/2000], Loss: 1737.7017\n",
            "Epoch [350/2000], Loss: 1708.7989\n",
            "Epoch [360/2000], Loss: 1679.9223\n",
            "Epoch [370/2000], Loss: 1651.0740\n",
            "Epoch [380/2000], Loss: 1622.2558\n",
            "Epoch [390/2000], Loss: 1593.4695\n",
            "Epoch [400/2000], Loss: 1564.7168\n",
            "Epoch [410/2000], Loss: 1535.9995\n",
            "Epoch [420/2000], Loss: 1507.3191\n",
            "Epoch [430/2000], Loss: 1478.6774\n",
            "Epoch [440/2000], Loss: 1450.0757\n",
            "Epoch [450/2000], Loss: 1421.5157\n",
            "Epoch [460/2000], Loss: 1392.9987\n",
            "Epoch [470/2000], Loss: 1364.5262\n",
            "Epoch [480/2000], Loss: 1336.0996\n",
            "Epoch [490/2000], Loss: 1307.7202\n",
            "Epoch [500/2000], Loss: 1279.3894\n",
            "Epoch [510/2000], Loss: 1251.1086\n",
            "Epoch [520/2000], Loss: 1222.8793\n",
            "Epoch [530/2000], Loss: 1194.7030\n",
            "Epoch [540/2000], Loss: 1166.5813\n",
            "Epoch [550/2000], Loss: 1138.5157\n",
            "Epoch [560/2000], Loss: 1110.5081\n",
            "Epoch [570/2000], Loss: 1082.5601\n",
            "Epoch [580/2000], Loss: 1054.6736\n",
            "Epoch [590/2000], Loss: 1026.8507\n",
            "Epoch [600/2000], Loss: 999.0932\n",
            "Epoch [610/2000], Loss: 971.4034\n",
            "Epoch [620/2000], Loss: 943.7836\n",
            "Epoch [630/2000], Loss: 916.2362\n",
            "Epoch [640/2000], Loss: 888.7640\n",
            "Epoch [650/2000], Loss: 861.3699\n",
            "Epoch [660/2000], Loss: 834.0574\n",
            "Epoch [670/2000], Loss: 806.8302\n",
            "Epoch [680/2000], Loss: 779.6922\n",
            "Epoch [690/2000], Loss: 752.6480\n",
            "Epoch [700/2000], Loss: 725.7021\n",
            "Epoch [710/2000], Loss: 698.8597\n",
            "Epoch [720/2000], Loss: 672.1263\n",
            "Epoch [730/2000], Loss: 645.5079\n",
            "Epoch [740/2000], Loss: 619.0113\n",
            "Epoch [750/2000], Loss: 592.6440\n",
            "Epoch [760/2000], Loss: 566.4148\n",
            "Epoch [770/2000], Loss: 540.3335\n",
            "Epoch [780/2000], Loss: 514.4115\n",
            "Epoch [790/2000], Loss: 488.6621\n",
            "Epoch [800/2000], Loss: 463.1016\n",
            "Epoch [810/2000], Loss: 437.7507\n",
            "Epoch [820/2000], Loss: 412.6368\n",
            "Epoch [830/2000], Loss: 387.7964\n",
            "Epoch [840/2000], Loss: 363.2786\n",
            "Epoch [850/2000], Loss: 339.1478\n",
            "Epoch [860/2000], Loss: 315.4902\n",
            "Epoch [870/2000], Loss: 292.4210\n",
            "Epoch [880/2000], Loss: 270.0959\n",
            "Epoch [890/2000], Loss: 248.7200\n",
            "Epoch [900/2000], Loss: 228.5586\n",
            "Epoch [910/2000], Loss: 209.9438\n",
            "Epoch [920/2000], Loss: 193.2792\n",
            "Epoch [930/2000], Loss: 179.0420\n",
            "Epoch [940/2000], Loss: 167.7232\n",
            "Epoch [950/2000], Loss: 159.6271\n",
            "Epoch [960/2000], Loss: 154.5988\n",
            "Epoch [970/2000], Loss: 152.0005\n",
            "Epoch [980/2000], Loss: 151.1370\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 2.9793\n",
            "Epoch [20/50000], Loss: 2.9707\n",
            "Epoch [30/50000], Loss: 2.9620\n",
            "Epoch [40/50000], Loss: 2.9536\n",
            "Epoch [50/50000], Loss: 2.9461\n",
            "Epoch [60/50000], Loss: 2.9398\n",
            "Epoch [70/50000], Loss: 2.9348\n",
            "Epoch [80/50000], Loss: 2.9311\n",
            "Epoch [90/50000], Loss: 2.9287\n",
            "Epoch [100/50000], Loss: 2.9275\n",
            "Epoch [110/50000], Loss: 2.9272\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9272113111137856\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 10 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 2132.7763\n",
            "Epoch [20/2000], Loss: 2113.0307\n",
            "Epoch [30/2000], Loss: 2091.1371\n",
            "Epoch [40/2000], Loss: 2067.4840\n",
            "Epoch [50/2000], Loss: 2042.4802\n",
            "Epoch [60/2000], Loss: 2016.4657\n",
            "Epoch [70/2000], Loss: 1989.6970\n",
            "Epoch [80/2000], Loss: 1962.3600\n",
            "Epoch [90/2000], Loss: 1934.5876\n",
            "Epoch [100/2000], Loss: 1906.4760\n",
            "Epoch [110/2000], Loss: 1878.0963\n",
            "Epoch [120/2000], Loss: 1849.5023\n",
            "Epoch [130/2000], Loss: 1820.7358\n",
            "Epoch [140/2000], Loss: 1791.8283\n",
            "Epoch [150/2000], Loss: 1762.8030\n",
            "Epoch [160/2000], Loss: 1733.6786\n",
            "Epoch [170/2000], Loss: 1704.4730\n",
            "Epoch [180/2000], Loss: 1675.2024\n",
            "Epoch [190/2000], Loss: 1645.8792\n",
            "Epoch [200/2000], Loss: 1616.5123\n",
            "Epoch [210/2000], Loss: 1587.1081\n",
            "Epoch [220/2000], Loss: 1557.6719\n",
            "Epoch [230/2000], Loss: 1528.2095\n",
            "Epoch [240/2000], Loss: 1498.7292\n",
            "Epoch [250/2000], Loss: 1469.2409\n",
            "Epoch [260/2000], Loss: 1439.7511\n",
            "Epoch [270/2000], Loss: 1410.2585\n",
            "Epoch [280/2000], Loss: 1380.7563\n",
            "Epoch [290/2000], Loss: 1351.2458\n",
            "Epoch [300/2000], Loss: 1321.7388\n",
            "Epoch [310/2000], Loss: 1292.2460\n",
            "Epoch [320/2000], Loss: 1262.7716\n",
            "Epoch [330/2000], Loss: 1233.3148\n",
            "Epoch [340/2000], Loss: 1203.8721\n",
            "Epoch [350/2000], Loss: 1174.4391\n",
            "Epoch [360/2000], Loss: 1145.0134\n",
            "Epoch [370/2000], Loss: 1115.5994\n",
            "Epoch [380/2000], Loss: 1086.2101\n",
            "Epoch [390/2000], Loss: 1056.8608\n",
            "Epoch [400/2000], Loss: 1027.5585\n",
            "Epoch [410/2000], Loss: 998.2943\n",
            "Epoch [420/2000], Loss: 969.0450\n",
            "Epoch [430/2000], Loss: 939.7920\n",
            "Epoch [440/2000], Loss: 910.5464\n",
            "Epoch [450/2000], Loss: 881.3364\n",
            "Epoch [460/2000], Loss: 852.1811\n",
            "Epoch [470/2000], Loss: 823.0867\n",
            "Epoch [480/2000], Loss: 794.0513\n",
            "Epoch [490/2000], Loss: 765.0690\n",
            "Epoch [500/2000], Loss: 736.1311\n",
            "Epoch [510/2000], Loss: 707.2284\n",
            "Epoch [520/2000], Loss: 678.3553\n",
            "Epoch [530/2000], Loss: 649.5169\n",
            "Epoch [540/2000], Loss: 620.7349\n",
            "Epoch [550/2000], Loss: 592.0442\n",
            "Epoch [560/2000], Loss: 563.4785\n",
            "Epoch [570/2000], Loss: 535.0552\n",
            "Epoch [580/2000], Loss: 506.7661\n",
            "Epoch [590/2000], Loss: 478.5765\n",
            "Epoch [600/2000], Loss: 450.4461\n",
            "Epoch [610/2000], Loss: 422.3806\n",
            "Epoch [620/2000], Loss: 394.4551\n",
            "Epoch [630/2000], Loss: 366.7678\n",
            "Epoch [640/2000], Loss: 339.4053\n",
            "Epoch [650/2000], Loss: 312.4451\n",
            "Epoch [660/2000], Loss: 285.9710\n",
            "Epoch [670/2000], Loss: 260.0865\n",
            "Epoch [680/2000], Loss: 234.9298\n",
            "Epoch [690/2000], Loss: 210.6998\n",
            "Epoch [700/2000], Loss: 187.7077\n",
            "Epoch [710/2000], Loss: 166.4745\n",
            "Epoch [720/2000], Loss: 147.8585\n",
            "Epoch [730/2000], Loss: 133.0778\n",
            "Epoch [740/2000], Loss: 123.3747\n",
            "Epoch [750/2000], Loss: 119.2304\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 2.9989\n",
            "Epoch [20/50000], Loss: 2.9893\n",
            "Epoch [30/50000], Loss: 2.9795\n",
            "Epoch [40/50000], Loss: 2.9697\n",
            "Epoch [50/50000], Loss: 2.9606\n",
            "Epoch [60/50000], Loss: 2.9524\n",
            "Epoch [70/50000], Loss: 2.9455\n",
            "Epoch [80/50000], Loss: 2.9400\n",
            "Epoch [90/50000], Loss: 2.9359\n",
            "Epoch [100/50000], Loss: 2.9332\n",
            "Epoch [110/50000], Loss: 2.9316\n",
            "Epoch [120/50000], Loss: 2.9310\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9309367759863654\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 11 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 376.3854\n",
            "Epoch [20/2000], Loss: 368.8215\n",
            "Epoch [30/2000], Loss: 360.4917\n",
            "Epoch [40/2000], Loss: 351.5651\n",
            "Epoch [50/2000], Loss: 342.2182\n",
            "Epoch [60/2000], Loss: 332.6008\n",
            "Epoch [70/2000], Loss: 322.8305\n",
            "Epoch [80/2000], Loss: 312.9987\n",
            "Epoch [90/2000], Loss: 303.1783\n",
            "Epoch [100/2000], Loss: 293.4299\n",
            "Epoch [110/2000], Loss: 283.8069\n",
            "Epoch [120/2000], Loss: 274.3598\n",
            "Epoch [130/2000], Loss: 265.1388\n",
            "Epoch [140/2000], Loss: 256.1981\n",
            "Epoch [150/2000], Loss: 247.5985\n",
            "Epoch [160/2000], Loss: 239.4117\n",
            "Epoch [170/2000], Loss: 231.7250\n",
            "Epoch [180/2000], Loss: 224.6494\n",
            "Epoch [190/2000], Loss: 218.3336\n",
            "Epoch [200/2000], Loss: 212.9871\n",
            "Epoch [210/2000], Loss: 208.9233\n",
            "Epoch [220/2000], Loss: 206.6423\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0548\n",
            "Epoch [20/50000], Loss: 3.0412\n",
            "Epoch [30/50000], Loss: 3.0273\n",
            "Epoch [40/50000], Loss: 3.0137\n",
            "Epoch [50/50000], Loss: 3.0011\n",
            "Epoch [60/50000], Loss: 2.9897\n",
            "Epoch [70/50000], Loss: 2.9799\n",
            "Epoch [80/50000], Loss: 2.9716\n",
            "Epoch [90/50000], Loss: 2.9644\n",
            "Epoch [100/50000], Loss: 2.9581\n",
            "Epoch [110/50000], Loss: 2.9525\n",
            "Epoch [120/50000], Loss: 2.9476\n",
            "Epoch [130/50000], Loss: 2.9435\n",
            "Epoch [140/50000], Loss: 2.9402\n",
            "Epoch [150/50000], Loss: 2.9374\n",
            "Epoch [160/50000], Loss: 2.9351\n",
            "Epoch [170/50000], Loss: 2.9332\n",
            "Epoch [180/50000], Loss: 2.9314\n",
            "Epoch [190/50000], Loss: 2.9297\n",
            "Epoch [200/50000], Loss: 2.9281\n",
            "Epoch [210/50000], Loss: 2.9266\n",
            "Epoch [220/50000], Loss: 2.9250\n",
            "Epoch [230/50000], Loss: 2.9234\n",
            "Epoch [240/50000], Loss: 2.9218\n",
            "Epoch [250/50000], Loss: 2.9204\n",
            "Epoch [260/50000], Loss: 2.9191\n",
            "Epoch [270/50000], Loss: 2.9179\n",
            "Epoch [280/50000], Loss: 2.9169\n",
            "Epoch [290/50000], Loss: 2.9161\n",
            "Epoch [300/50000], Loss: 2.9155\n",
            "Epoch [310/50000], Loss: 2.9152\n",
            "Epoch [320/50000], Loss: 2.9149\n",
            "Epoch [330/50000], Loss: 2.9149\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.91485524579274\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 12 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 2631.2478\n",
            "Epoch [20/2000], Loss: 2611.7504\n",
            "Epoch [30/2000], Loss: 2590.1354\n",
            "Epoch [40/2000], Loss: 2566.7841\n",
            "Epoch [50/2000], Loss: 2542.0989\n",
            "Epoch [60/2000], Loss: 2516.4152\n",
            "Epoch [70/2000], Loss: 2489.9864\n",
            "Epoch [80/2000], Loss: 2462.9964\n",
            "Epoch [90/2000], Loss: 2435.5768\n",
            "Epoch [100/2000], Loss: 2407.8231\n",
            "Epoch [110/2000], Loss: 2379.8051\n",
            "Epoch [120/2000], Loss: 2351.5752\n",
            "Epoch [130/2000], Loss: 2323.1734\n",
            "Epoch [140/2000], Loss: 2294.6311\n",
            "Epoch [150/2000], Loss: 2265.9730\n",
            "Epoch [160/2000], Loss: 2237.2191\n",
            "Epoch [170/2000], Loss: 2208.3859\n",
            "Epoch [180/2000], Loss: 2179.4870\n",
            "Epoch [190/2000], Loss: 2150.5336\n",
            "Epoch [200/2000], Loss: 2121.5355\n",
            "Epoch [210/2000], Loss: 2092.5009\n",
            "Epoch [220/2000], Loss: 2063.4369\n",
            "Epoch [230/2000], Loss: 2034.3496\n",
            "Epoch [240/2000], Loss: 2005.2444\n",
            "Epoch [250/2000], Loss: 1976.1260\n",
            "Epoch [260/2000], Loss: 1946.9986\n",
            "Epoch [270/2000], Loss: 1917.8660\n",
            "Epoch [280/2000], Loss: 1888.7314\n",
            "Epoch [290/2000], Loss: 1859.5979\n",
            "Epoch [300/2000], Loss: 1830.4682\n",
            "Epoch [310/2000], Loss: 1801.3447\n",
            "Epoch [320/2000], Loss: 1772.2296\n",
            "Epoch [330/2000], Loss: 1743.1251\n",
            "Epoch [340/2000], Loss: 1714.0330\n",
            "Epoch [350/2000], Loss: 1684.9550\n",
            "Epoch [360/2000], Loss: 1655.8929\n",
            "Epoch [370/2000], Loss: 1626.8480\n",
            "Epoch [380/2000], Loss: 1597.8220\n",
            "Epoch [390/2000], Loss: 1568.8160\n",
            "Epoch [400/2000], Loss: 1539.8315\n",
            "Epoch [410/2000], Loss: 1510.8695\n",
            "Epoch [420/2000], Loss: 1481.9313\n",
            "Epoch [430/2000], Loss: 1453.0181\n",
            "Epoch [440/2000], Loss: 1424.1309\n",
            "Epoch [450/2000], Loss: 1395.2707\n",
            "Epoch [460/2000], Loss: 1366.4387\n",
            "Epoch [470/2000], Loss: 1337.6358\n",
            "Epoch [480/2000], Loss: 1308.8632\n",
            "Epoch [490/2000], Loss: 1280.1217\n",
            "Epoch [500/2000], Loss: 1251.4124\n",
            "Epoch [510/2000], Loss: 1222.7365\n",
            "Epoch [520/2000], Loss: 1194.0948\n",
            "Epoch [530/2000], Loss: 1165.4886\n",
            "Epoch [540/2000], Loss: 1136.9190\n",
            "Epoch [550/2000], Loss: 1108.3870\n",
            "Epoch [560/2000], Loss: 1079.8940\n",
            "Epoch [570/2000], Loss: 1051.4412\n",
            "Epoch [580/2000], Loss: 1023.0300\n",
            "Epoch [590/2000], Loss: 994.6619\n",
            "Epoch [600/2000], Loss: 966.3384\n",
            "Epoch [610/2000], Loss: 938.0613\n",
            "Epoch [620/2000], Loss: 909.8326\n",
            "Epoch [630/2000], Loss: 881.6542\n",
            "Epoch [640/2000], Loss: 853.5284\n",
            "Epoch [650/2000], Loss: 825.4580\n",
            "Epoch [660/2000], Loss: 797.4457\n",
            "Epoch [670/2000], Loss: 769.4948\n",
            "Epoch [680/2000], Loss: 741.6089\n",
            "Epoch [690/2000], Loss: 713.7924\n",
            "Epoch [700/2000], Loss: 686.0498\n",
            "Epoch [710/2000], Loss: 658.3868\n",
            "Epoch [720/2000], Loss: 630.8097\n",
            "Epoch [730/2000], Loss: 603.3260\n",
            "Epoch [740/2000], Loss: 575.9443\n",
            "Epoch [750/2000], Loss: 548.6751\n",
            "Epoch [760/2000], Loss: 521.5308\n",
            "Epoch [770/2000], Loss: 494.5266\n",
            "Epoch [780/2000], Loss: 467.6808\n",
            "Epoch [790/2000], Loss: 441.0160\n",
            "Epoch [800/2000], Loss: 414.5607\n",
            "Epoch [810/2000], Loss: 388.3503\n",
            "Epoch [820/2000], Loss: 362.4303\n",
            "Epoch [830/2000], Loss: 336.8591\n",
            "Epoch [840/2000], Loss: 311.7119\n",
            "Epoch [850/2000], Loss: 287.0881\n",
            "Epoch [860/2000], Loss: 263.1201\n",
            "Epoch [870/2000], Loss: 239.9889\n",
            "Epoch [880/2000], Loss: 217.9466\n",
            "Epoch [890/2000], Loss: 197.3481\n",
            "Epoch [900/2000], Loss: 178.6844\n",
            "Epoch [910/2000], Loss: 162.5926\n",
            "Epoch [920/2000], Loss: 149.7890\n",
            "Epoch [930/2000], Loss: 140.8984\n",
            "Epoch [940/2000], Loss: 136.1295\n",
            "Epoch [950/2000], Loss: 134.8143\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0490\n",
            "Epoch [20/50000], Loss: 3.0370\n",
            "Epoch [30/50000], Loss: 3.0249\n",
            "Epoch [40/50000], Loss: 3.0134\n",
            "Epoch [50/50000], Loss: 3.0026\n",
            "Epoch [60/50000], Loss: 2.9929\n",
            "Epoch [70/50000], Loss: 2.9841\n",
            "Epoch [80/50000], Loss: 2.9758\n",
            "Epoch [90/50000], Loss: 2.9679\n",
            "Epoch [100/50000], Loss: 2.9605\n",
            "Epoch [110/50000], Loss: 2.9539\n",
            "Epoch [120/50000], Loss: 2.9482\n",
            "Epoch [130/50000], Loss: 2.9432\n",
            "Epoch [140/50000], Loss: 2.9391\n",
            "Epoch [150/50000], Loss: 2.9356\n",
            "Epoch [160/50000], Loss: 2.9327\n",
            "Epoch [170/50000], Loss: 2.9303\n",
            "Epoch [180/50000], Loss: 2.9282\n",
            "Epoch [190/50000], Loss: 2.9264\n",
            "Epoch [200/50000], Loss: 2.9249\n",
            "Epoch [210/50000], Loss: 2.9236\n",
            "Epoch [220/50000], Loss: 2.9224\n",
            "Epoch [230/50000], Loss: 2.9214\n",
            "Epoch [240/50000], Loss: 2.9205\n",
            "Epoch [250/50000], Loss: 2.9197\n",
            "Epoch [260/50000], Loss: 2.9191\n",
            "Epoch [270/50000], Loss: 2.9186\n",
            "Epoch [280/50000], Loss: 2.9182\n",
            "Epoch [290/50000], Loss: 2.9179\n",
            "Epoch [300/50000], Loss: 2.9176\n",
            "Epoch [310/50000], Loss: 2.9173\n",
            "Epoch [320/50000], Loss: 2.9171\n",
            "Epoch [330/50000], Loss: 2.9168\n",
            "Epoch [340/50000], Loss: 2.9165\n",
            "Epoch [350/50000], Loss: 2.9162\n",
            "Epoch [360/50000], Loss: 2.9159\n",
            "Epoch [370/50000], Loss: 2.9155\n",
            "Epoch [380/50000], Loss: 2.9153\n",
            "Epoch [390/50000], Loss: 2.9151\n",
            "Epoch [400/50000], Loss: 2.9149\n",
            "Epoch [410/50000], Loss: 2.9148\n",
            "Epoch [420/50000], Loss: 2.9148\n",
            "Epoch [430/50000], Loss: 2.9148\n",
            "Epoch [440/50000], Loss: 2.9148\n",
            "Epoch [450/50000], Loss: 2.9147\n",
            "Epoch [460/50000], Loss: 2.9145\n",
            "Epoch [470/50000], Loss: 2.9142\n",
            "Epoch [480/50000], Loss: 2.9139\n",
            "Epoch [490/50000], Loss: 2.9135\n",
            "Epoch [500/50000], Loss: 2.9132\n",
            "Epoch [510/50000], Loss: 2.9128\n",
            "Epoch [520/50000], Loss: 2.9126\n",
            "Epoch [530/50000], Loss: 2.9123\n",
            "Epoch [540/50000], Loss: 2.9122\n",
            "Epoch [550/50000], Loss: 2.9121\n",
            "Epoch [560/50000], Loss: 2.9120\n",
            "Epoch [570/50000], Loss: 2.9120\n",
            "Epoch [580/50000], Loss: 2.9119\n",
            "Epoch [590/50000], Loss: 2.9118\n",
            "Epoch [600/50000], Loss: 2.9116\n",
            "Epoch [610/50000], Loss: 2.9113\n",
            "Epoch [620/50000], Loss: 2.9110\n",
            "Epoch [630/50000], Loss: 2.9107\n",
            "Epoch [640/50000], Loss: 2.9104\n",
            "Epoch [650/50000], Loss: 2.9101\n",
            "Epoch [660/50000], Loss: 2.9100\n",
            "Epoch [670/50000], Loss: 2.9099\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.909849518767217\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 13 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 395.9860\n",
            "Epoch [20/2000], Loss: 388.3955\n",
            "Epoch [30/2000], Loss: 380.0298\n",
            "Epoch [40/2000], Loss: 371.0555\n",
            "Epoch [50/2000], Loss: 361.6470\n",
            "Epoch [60/2000], Loss: 351.9513\n",
            "Epoch [70/2000], Loss: 342.0836\n",
            "Epoch [80/2000], Loss: 332.1320\n",
            "Epoch [90/2000], Loss: 322.1653\n",
            "Epoch [100/2000], Loss: 312.2395\n",
            "Epoch [110/2000], Loss: 302.4025\n",
            "Epoch [120/2000], Loss: 292.6986\n",
            "Epoch [130/2000], Loss: 283.1712\n",
            "Epoch [140/2000], Loss: 273.8658\n",
            "Epoch [150/2000], Loss: 264.8324\n",
            "Epoch [160/2000], Loss: 256.1292\n",
            "Epoch [170/2000], Loss: 247.8255\n",
            "Epoch [180/2000], Loss: 240.0044\n",
            "Epoch [190/2000], Loss: 232.7716\n",
            "Epoch [200/2000], Loss: 226.2720\n",
            "Epoch [210/2000], Loss: 220.7154\n",
            "Epoch [220/2000], Loss: 216.4292\n",
            "Epoch [230/2000], Loss: 213.9699\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.1012\n",
            "Epoch [20/50000], Loss: 3.0854\n",
            "Epoch [30/50000], Loss: 3.0692\n",
            "Epoch [40/50000], Loss: 3.0533\n",
            "Epoch [50/50000], Loss: 3.0382\n",
            "Epoch [60/50000], Loss: 3.0245\n",
            "Epoch [70/50000], Loss: 3.0121\n",
            "Epoch [80/50000], Loss: 3.0013\n",
            "Epoch [90/50000], Loss: 2.9920\n",
            "Epoch [100/50000], Loss: 2.9838\n",
            "Epoch [110/50000], Loss: 2.9767\n",
            "Epoch [120/50000], Loss: 2.9704\n",
            "Epoch [130/50000], Loss: 2.9646\n",
            "Epoch [140/50000], Loss: 2.9592\n",
            "Epoch [150/50000], Loss: 2.9541\n",
            "Epoch [160/50000], Loss: 2.9493\n",
            "Epoch [170/50000], Loss: 2.9448\n",
            "Epoch [180/50000], Loss: 2.9408\n",
            "Epoch [190/50000], Loss: 2.9371\n",
            "Epoch [200/50000], Loss: 2.9339\n",
            "Epoch [210/50000], Loss: 2.9311\n",
            "Epoch [220/50000], Loss: 2.9286\n",
            "Epoch [230/50000], Loss: 2.9265\n",
            "Epoch [240/50000], Loss: 2.9247\n",
            "Epoch [250/50000], Loss: 2.9231\n",
            "Epoch [260/50000], Loss: 2.9218\n",
            "Epoch [270/50000], Loss: 2.9206\n",
            "Epoch [280/50000], Loss: 2.9196\n",
            "Epoch [290/50000], Loss: 2.9187\n",
            "Epoch [300/50000], Loss: 2.9179\n",
            "Epoch [310/50000], Loss: 2.9171\n",
            "Epoch [320/50000], Loss: 2.9165\n",
            "Epoch [330/50000], Loss: 2.9159\n",
            "Epoch [340/50000], Loss: 2.9153\n",
            "Epoch [350/50000], Loss: 2.9147\n",
            "Epoch [360/50000], Loss: 2.9142\n",
            "Epoch [370/50000], Loss: 2.9136\n",
            "Epoch [380/50000], Loss: 2.9131\n",
            "Epoch [390/50000], Loss: 2.9125\n",
            "Epoch [400/50000], Loss: 2.9120\n",
            "Epoch [410/50000], Loss: 2.9115\n",
            "Epoch [420/50000], Loss: 2.9110\n",
            "Epoch [430/50000], Loss: 2.9106\n",
            "Epoch [440/50000], Loss: 2.9102\n",
            "Epoch [450/50000], Loss: 2.9098\n",
            "Epoch [460/50000], Loss: 2.9094\n",
            "Epoch [470/50000], Loss: 2.9090\n",
            "Epoch [480/50000], Loss: 2.9087\n",
            "Epoch [490/50000], Loss: 2.9084\n",
            "Epoch [500/50000], Loss: 2.9081\n",
            "Epoch [510/50000], Loss: 2.9078\n",
            "Epoch [520/50000], Loss: 2.9076\n",
            "Epoch [530/50000], Loss: 2.9073\n",
            "Epoch [540/50000], Loss: 2.9071\n",
            "Epoch [550/50000], Loss: 2.9069\n",
            "Epoch [560/50000], Loss: 2.9068\n",
            "Epoch [570/50000], Loss: 2.9067\n",
            "Epoch [580/50000], Loss: 2.9066\n",
            "Epoch [590/50000], Loss: 2.9065\n",
            "Epoch [600/50000], Loss: 2.9065\n",
            "Epoch [610/50000], Loss: 2.9064\n",
            "Epoch [620/50000], Loss: 2.9064\n",
            "Epoch [630/50000], Loss: 2.9064\n",
            "Epoch [640/50000], Loss: 2.9064\n",
            "Epoch [650/50000], Loss: 2.9063\n",
            "Epoch [660/50000], Loss: 2.9063\n",
            "Epoch [670/50000], Loss: 2.9063\n",
            "Epoch [680/50000], Loss: 2.9062\n",
            "Epoch [690/50000], Loss: 2.9061\n",
            "Epoch [700/50000], Loss: 2.9061\n",
            "Epoch [710/50000], Loss: 2.9060\n",
            "Epoch [720/50000], Loss: 2.9060\n",
            "Epoch [730/50000], Loss: 2.9059\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9059396130962094\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 14 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 924.5058\n",
            "Epoch [20/2000], Loss: 916.3832\n",
            "Epoch [30/2000], Loss: 907.3865\n",
            "Epoch [40/2000], Loss: 897.6771\n",
            "Epoch [50/2000], Loss: 887.4248\n",
            "Epoch [60/2000], Loss: 876.7712\n",
            "Epoch [70/2000], Loss: 865.8233\n",
            "Epoch [80/2000], Loss: 854.6588\n",
            "Epoch [90/2000], Loss: 843.3337\n",
            "Epoch [100/2000], Loss: 831.8887\n",
            "Epoch [110/2000], Loss: 820.3538\n",
            "Epoch [120/2000], Loss: 808.7518\n",
            "Epoch [130/2000], Loss: 797.1001\n",
            "Epoch [140/2000], Loss: 785.4127\n",
            "Epoch [150/2000], Loss: 773.7007\n",
            "Epoch [160/2000], Loss: 761.9734\n",
            "Epoch [170/2000], Loss: 750.2385\n",
            "Epoch [180/2000], Loss: 738.5025\n",
            "Epoch [190/2000], Loss: 726.7710\n",
            "Epoch [200/2000], Loss: 715.0489\n",
            "Epoch [210/2000], Loss: 703.3407\n",
            "Epoch [220/2000], Loss: 691.6502\n",
            "Epoch [230/2000], Loss: 679.9809\n",
            "Epoch [240/2000], Loss: 668.3360\n",
            "Epoch [250/2000], Loss: 656.7184\n",
            "Epoch [260/2000], Loss: 645.1308\n",
            "Epoch [270/2000], Loss: 633.5758\n",
            "Epoch [280/2000], Loss: 622.0557\n",
            "Epoch [290/2000], Loss: 610.5727\n",
            "Epoch [300/2000], Loss: 599.1290\n",
            "Epoch [310/2000], Loss: 587.7265\n",
            "Epoch [320/2000], Loss: 576.3670\n",
            "Epoch [330/2000], Loss: 565.0525\n",
            "Epoch [340/2000], Loss: 553.7846\n",
            "Epoch [350/2000], Loss: 542.5650\n",
            "Epoch [360/2000], Loss: 531.3951\n",
            "Epoch [370/2000], Loss: 520.2765\n",
            "Epoch [380/2000], Loss: 509.2106\n",
            "Epoch [390/2000], Loss: 498.1985\n",
            "Epoch [400/2000], Loss: 487.2414\n",
            "Epoch [410/2000], Loss: 476.3403\n",
            "Epoch [420/2000], Loss: 465.4960\n",
            "Epoch [430/2000], Loss: 454.7093\n",
            "Epoch [440/2000], Loss: 443.9805\n",
            "Epoch [450/2000], Loss: 433.3098\n",
            "Epoch [460/2000], Loss: 422.6973\n",
            "Epoch [470/2000], Loss: 412.1426\n",
            "Epoch [480/2000], Loss: 401.6451\n",
            "Epoch [490/2000], Loss: 391.2038\n",
            "Epoch [500/2000], Loss: 380.8173\n",
            "Epoch [510/2000], Loss: 370.4842\n",
            "Epoch [520/2000], Loss: 360.2022\n",
            "Epoch [530/2000], Loss: 349.9692\n",
            "Epoch [540/2000], Loss: 339.7825\n",
            "Epoch [550/2000], Loss: 329.6394\n",
            "Epoch [560/2000], Loss: 319.5369\n",
            "Epoch [570/2000], Loss: 309.4723\n",
            "Epoch [580/2000], Loss: 299.4430\n",
            "Epoch [590/2000], Loss: 289.4470\n",
            "Epoch [600/2000], Loss: 279.4832\n",
            "Epoch [610/2000], Loss: 269.5517\n",
            "Epoch [620/2000], Loss: 259.6541\n",
            "Epoch [630/2000], Loss: 249.7945\n",
            "Epoch [640/2000], Loss: 239.9794\n",
            "Epoch [650/2000], Loss: 230.2187\n",
            "Epoch [660/2000], Loss: 220.5256\n",
            "Epoch [670/2000], Loss: 210.9176\n",
            "Epoch [680/2000], Loss: 201.4164\n",
            "Epoch [690/2000], Loss: 192.0483\n",
            "Epoch [700/2000], Loss: 182.8447\n",
            "Epoch [710/2000], Loss: 173.8422\n",
            "Epoch [720/2000], Loss: 165.0834\n",
            "Epoch [730/2000], Loss: 156.6175\n",
            "Epoch [740/2000], Loss: 148.5019\n",
            "Epoch [750/2000], Loss: 140.8032\n",
            "Epoch [760/2000], Loss: 133.5999\n",
            "Epoch [770/2000], Loss: 126.9837\n",
            "Epoch [780/2000], Loss: 121.0620\n",
            "Epoch [790/2000], Loss: 115.9617\n",
            "Epoch [800/2000], Loss: 111.8411\n",
            "Epoch [810/2000], Loss: 108.9137\n",
            "Epoch [820/2000], Loss: 107.5037\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0943\n",
            "Epoch [20/50000], Loss: 3.0791\n",
            "Epoch [30/50000], Loss: 3.0635\n",
            "Epoch [40/50000], Loss: 3.0481\n",
            "Epoch [50/50000], Loss: 3.0336\n",
            "Epoch [60/50000], Loss: 3.0202\n",
            "Epoch [70/50000], Loss: 3.0082\n",
            "Epoch [80/50000], Loss: 2.9976\n",
            "Epoch [90/50000], Loss: 2.9883\n",
            "Epoch [100/50000], Loss: 2.9804\n",
            "Epoch [110/50000], Loss: 2.9736\n",
            "Epoch [120/50000], Loss: 2.9678\n",
            "Epoch [130/50000], Loss: 2.9628\n",
            "Epoch [140/50000], Loss: 2.9583\n",
            "Epoch [150/50000], Loss: 2.9541\n",
            "Epoch [160/50000], Loss: 2.9502\n",
            "Epoch [170/50000], Loss: 2.9464\n",
            "Epoch [180/50000], Loss: 2.9429\n",
            "Epoch [190/50000], Loss: 2.9396\n",
            "Epoch [200/50000], Loss: 2.9368\n",
            "Epoch [210/50000], Loss: 2.9342\n",
            "Epoch [220/50000], Loss: 2.9321\n",
            "Epoch [230/50000], Loss: 2.9303\n",
            "Epoch [240/50000], Loss: 2.9287\n",
            "Epoch [250/50000], Loss: 2.9274\n",
            "Epoch [260/50000], Loss: 2.9263\n",
            "Epoch [270/50000], Loss: 2.9254\n",
            "Epoch [280/50000], Loss: 2.9246\n",
            "Epoch [290/50000], Loss: 2.9239\n",
            "Epoch [300/50000], Loss: 2.9232\n",
            "Epoch [310/50000], Loss: 2.9226\n",
            "Epoch [320/50000], Loss: 2.9221\n",
            "Epoch [330/50000], Loss: 2.9216\n",
            "Epoch [340/50000], Loss: 2.9212\n",
            "Epoch [350/50000], Loss: 2.9207\n",
            "Epoch [360/50000], Loss: 2.9203\n",
            "Epoch [370/50000], Loss: 2.9200\n",
            "Epoch [380/50000], Loss: 2.9196\n",
            "Epoch [390/50000], Loss: 2.9193\n",
            "Epoch [400/50000], Loss: 2.9190\n",
            "Epoch [410/50000], Loss: 2.9188\n",
            "Epoch [420/50000], Loss: 2.9185\n",
            "Epoch [430/50000], Loss: 2.9183\n",
            "Epoch [440/50000], Loss: 2.9180\n",
            "Epoch [450/50000], Loss: 2.9178\n",
            "Epoch [460/50000], Loss: 2.9176\n",
            "Epoch [470/50000], Loss: 2.9174\n",
            "Epoch [480/50000], Loss: 2.9172\n",
            "Epoch [490/50000], Loss: 2.9170\n",
            "Epoch [500/50000], Loss: 2.9168\n",
            "Epoch [510/50000], Loss: 2.9166\n",
            "Epoch [520/50000], Loss: 2.9164\n",
            "Epoch [530/50000], Loss: 2.9163\n",
            "Epoch [540/50000], Loss: 2.9161\n",
            "Epoch [550/50000], Loss: 2.9159\n",
            "Epoch [560/50000], Loss: 2.9157\n",
            "Epoch [570/50000], Loss: 2.9155\n",
            "Epoch [580/50000], Loss: 2.9153\n",
            "Epoch [590/50000], Loss: 2.9152\n",
            "Epoch [600/50000], Loss: 2.9150\n",
            "Epoch [610/50000], Loss: 2.9149\n",
            "Epoch [620/50000], Loss: 2.9147\n",
            "Epoch [630/50000], Loss: 2.9146\n",
            "Epoch [640/50000], Loss: 2.9144\n",
            "Epoch [650/50000], Loss: 2.9142\n",
            "Epoch [660/50000], Loss: 2.9141\n",
            "Epoch [670/50000], Loss: 2.9139\n",
            "Epoch [680/50000], Loss: 2.9137\n",
            "Epoch [690/50000], Loss: 2.9136\n",
            "Epoch [700/50000], Loss: 2.9134\n",
            "Epoch [710/50000], Loss: 2.9133\n",
            "Epoch [720/50000], Loss: 2.9132\n",
            "Epoch [730/50000], Loss: 2.9131\n",
            "Epoch [740/50000], Loss: 2.9129\n",
            "Epoch [750/50000], Loss: 2.9128\n",
            "Epoch [760/50000], Loss: 2.9127\n",
            "Epoch [770/50000], Loss: 2.9125\n",
            "Epoch [780/50000], Loss: 2.9124\n",
            "Epoch [790/50000], Loss: 2.9122\n",
            "Epoch [800/50000], Loss: 2.9121\n",
            "Epoch [810/50000], Loss: 2.9120\n",
            "Epoch [820/50000], Loss: 2.9119\n",
            "Epoch [830/50000], Loss: 2.9119\n",
            "Epoch [840/50000], Loss: 2.9118\n",
            "Epoch [850/50000], Loss: 2.9117\n",
            "Epoch [860/50000], Loss: 2.9116\n",
            "Epoch [870/50000], Loss: 2.9115\n",
            "Epoch [880/50000], Loss: 2.9114\n",
            "Epoch [890/50000], Loss: 2.9113\n",
            "Epoch [900/50000], Loss: 2.9111\n",
            "Epoch [910/50000], Loss: 2.9110\n",
            "Epoch [920/50000], Loss: 2.9110\n",
            "Epoch [930/50000], Loss: 2.9109\n",
            "Epoch [940/50000], Loss: 2.9109\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9109182232643342\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 15 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 323.7949\n",
            "Epoch [20/2000], Loss: 316.9482\n",
            "Epoch [30/2000], Loss: 309.4201\n",
            "Epoch [40/2000], Loss: 301.3683\n",
            "Epoch [50/2000], Loss: 292.9570\n",
            "Epoch [60/2000], Loss: 284.3265\n",
            "Epoch [70/2000], Loss: 275.5885\n",
            "Epoch [80/2000], Loss: 266.8313\n",
            "Epoch [90/2000], Loss: 258.1278\n",
            "Epoch [100/2000], Loss: 249.5418\n",
            "Epoch [110/2000], Loss: 241.1329\n",
            "Epoch [120/2000], Loss: 232.9603\n",
            "Epoch [130/2000], Loss: 225.0847\n",
            "Epoch [140/2000], Loss: 217.5697\n",
            "Epoch [150/2000], Loss: 210.4863\n",
            "Epoch [160/2000], Loss: 203.9174\n",
            "Epoch [170/2000], Loss: 197.9644\n",
            "Epoch [180/2000], Loss: 192.7557\n",
            "Epoch [190/2000], Loss: 188.4584\n",
            "Epoch [200/2000], Loss: 185.2991\n",
            "Epoch [210/2000], Loss: 183.6228\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.0748\n",
            "Epoch [20/50000], Loss: 3.0612\n",
            "Epoch [30/50000], Loss: 3.0474\n",
            "Epoch [40/50000], Loss: 3.0340\n",
            "Epoch [50/50000], Loss: 3.0215\n",
            "Epoch [60/50000], Loss: 3.0102\n",
            "Epoch [70/50000], Loss: 3.0003\n",
            "Epoch [80/50000], Loss: 2.9916\n",
            "Epoch [90/50000], Loss: 2.9841\n",
            "Epoch [100/50000], Loss: 2.9777\n",
            "Epoch [110/50000], Loss: 2.9721\n",
            "Epoch [120/50000], Loss: 2.9670\n",
            "Epoch [130/50000], Loss: 2.9624\n",
            "Epoch [140/50000], Loss: 2.9581\n",
            "Epoch [150/50000], Loss: 2.9541\n",
            "Epoch [160/50000], Loss: 2.9505\n",
            "Epoch [170/50000], Loss: 2.9472\n",
            "Epoch [180/50000], Loss: 2.9443\n",
            "Epoch [190/50000], Loss: 2.9417\n",
            "Epoch [200/50000], Loss: 2.9395\n",
            "Epoch [210/50000], Loss: 2.9375\n",
            "Epoch [220/50000], Loss: 2.9358\n",
            "Epoch [230/50000], Loss: 2.9342\n",
            "Epoch [240/50000], Loss: 2.9328\n",
            "Epoch [250/50000], Loss: 2.9315\n",
            "Epoch [260/50000], Loss: 2.9303\n",
            "Epoch [270/50000], Loss: 2.9292\n",
            "Epoch [280/50000], Loss: 2.9281\n",
            "Epoch [290/50000], Loss: 2.9271\n",
            "Epoch [300/50000], Loss: 2.9261\n",
            "Epoch [310/50000], Loss: 2.9251\n",
            "Epoch [320/50000], Loss: 2.9242\n",
            "Epoch [330/50000], Loss: 2.9233\n",
            "Epoch [340/50000], Loss: 2.9224\n",
            "Epoch [350/50000], Loss: 2.9216\n",
            "Epoch [360/50000], Loss: 2.9208\n",
            "Epoch [370/50000], Loss: 2.9200\n",
            "Epoch [380/50000], Loss: 2.9193\n",
            "Epoch [390/50000], Loss: 2.9187\n",
            "Epoch [400/50000], Loss: 2.9180\n",
            "Epoch [410/50000], Loss: 2.9174\n",
            "Epoch [420/50000], Loss: 2.9168\n",
            "Epoch [430/50000], Loss: 2.9161\n",
            "Epoch [440/50000], Loss: 2.9155\n",
            "Epoch [450/50000], Loss: 2.9149\n",
            "Epoch [460/50000], Loss: 2.9143\n",
            "Epoch [470/50000], Loss: 2.9137\n",
            "Epoch [480/50000], Loss: 2.9131\n",
            "Epoch [490/50000], Loss: 2.9126\n",
            "Epoch [500/50000], Loss: 2.9121\n",
            "Epoch [510/50000], Loss: 2.9117\n",
            "Epoch [520/50000], Loss: 2.9113\n",
            "Epoch [530/50000], Loss: 2.9109\n",
            "Epoch [540/50000], Loss: 2.9105\n",
            "Epoch [550/50000], Loss: 2.9101\n",
            "Epoch [560/50000], Loss: 2.9096\n",
            "Epoch [570/50000], Loss: 2.9092\n",
            "Epoch [580/50000], Loss: 2.9087\n",
            "Epoch [590/50000], Loss: 2.9083\n",
            "Epoch [600/50000], Loss: 2.9079\n",
            "Epoch [610/50000], Loss: 2.9075\n",
            "Epoch [620/50000], Loss: 2.9072\n",
            "Epoch [630/50000], Loss: 2.9069\n",
            "Epoch [640/50000], Loss: 2.9067\n",
            "Epoch [650/50000], Loss: 2.9065\n",
            "Epoch [660/50000], Loss: 2.9064\n",
            "Epoch [670/50000], Loss: 2.9062\n",
            "Epoch [680/50000], Loss: 2.9061\n",
            "Epoch [690/50000], Loss: 2.9059\n",
            "Epoch [700/50000], Loss: 2.9058\n",
            "Epoch [710/50000], Loss: 2.9057\n",
            "Epoch [720/50000], Loss: 2.9057\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.905684717488207\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 16 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 230.2024\n",
            "Epoch [20/2000], Loss: 224.2185\n",
            "Epoch [30/2000], Loss: 217.8009\n",
            "Epoch [40/2000], Loss: 211.1642\n",
            "Epoch [50/2000], Loss: 204.5411\n",
            "Epoch [60/2000], Loss: 198.1599\n",
            "Epoch [70/2000], Loss: 192.2525\n",
            "Epoch [80/2000], Loss: 187.0799\n",
            "Epoch [90/2000], Loss: 182.9776\n",
            "Epoch [100/2000], Loss: 180.4294\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.1166\n",
            "Epoch [20/50000], Loss: 3.0998\n",
            "Epoch [30/50000], Loss: 3.0826\n",
            "Epoch [40/50000], Loss: 3.0658\n",
            "Epoch [50/50000], Loss: 3.0499\n",
            "Epoch [60/50000], Loss: 3.0353\n",
            "Epoch [70/50000], Loss: 3.0221\n",
            "Epoch [80/50000], Loss: 3.0104\n",
            "Epoch [90/50000], Loss: 3.0002\n",
            "Epoch [100/50000], Loss: 2.9915\n",
            "Epoch [110/50000], Loss: 2.9840\n",
            "Epoch [120/50000], Loss: 2.9777\n",
            "Epoch [130/50000], Loss: 2.9723\n",
            "Epoch [140/50000], Loss: 2.9676\n",
            "Epoch [150/50000], Loss: 2.9634\n",
            "Epoch [160/50000], Loss: 2.9596\n",
            "Epoch [170/50000], Loss: 2.9561\n",
            "Epoch [180/50000], Loss: 2.9528\n",
            "Epoch [190/50000], Loss: 2.9496\n",
            "Epoch [200/50000], Loss: 2.9466\n",
            "Epoch [210/50000], Loss: 2.9438\n",
            "Epoch [220/50000], Loss: 2.9412\n",
            "Epoch [230/50000], Loss: 2.9388\n",
            "Epoch [240/50000], Loss: 2.9366\n",
            "Epoch [250/50000], Loss: 2.9346\n",
            "Epoch [260/50000], Loss: 2.9329\n",
            "Epoch [270/50000], Loss: 2.9313\n",
            "Epoch [280/50000], Loss: 2.9299\n",
            "Epoch [290/50000], Loss: 2.9286\n",
            "Epoch [300/50000], Loss: 2.9274\n",
            "Epoch [310/50000], Loss: 2.9262\n",
            "Epoch [320/50000], Loss: 2.9251\n",
            "Epoch [330/50000], Loss: 2.9241\n",
            "Epoch [340/50000], Loss: 2.9231\n",
            "Epoch [350/50000], Loss: 2.9221\n",
            "Epoch [360/50000], Loss: 2.9211\n",
            "Epoch [370/50000], Loss: 2.9202\n",
            "Epoch [380/50000], Loss: 2.9193\n",
            "Epoch [390/50000], Loss: 2.9185\n",
            "Epoch [400/50000], Loss: 2.9177\n",
            "Epoch [410/50000], Loss: 2.9170\n",
            "Epoch [420/50000], Loss: 2.9163\n",
            "Epoch [430/50000], Loss: 2.9157\n",
            "Epoch [440/50000], Loss: 2.9151\n",
            "Epoch [450/50000], Loss: 2.9146\n",
            "Epoch [460/50000], Loss: 2.9140\n",
            "Epoch [470/50000], Loss: 2.9135\n",
            "Epoch [480/50000], Loss: 2.9130\n",
            "Epoch [490/50000], Loss: 2.9125\n",
            "Epoch [500/50000], Loss: 2.9121\n",
            "Epoch [510/50000], Loss: 2.9117\n",
            "Epoch [520/50000], Loss: 2.9113\n",
            "Epoch [530/50000], Loss: 2.9110\n",
            "Epoch [540/50000], Loss: 2.9107\n",
            "Epoch [550/50000], Loss: 2.9105\n",
            "Epoch [560/50000], Loss: 2.9103\n",
            "Epoch [570/50000], Loss: 2.9100\n",
            "Epoch [580/50000], Loss: 2.9098\n",
            "Epoch [590/50000], Loss: 2.9096\n",
            "Epoch [600/50000], Loss: 2.9093\n",
            "Epoch [610/50000], Loss: 2.9091\n",
            "Epoch [620/50000], Loss: 2.9088\n",
            "Epoch [630/50000], Loss: 2.9086\n",
            "Epoch [640/50000], Loss: 2.9084\n",
            "Epoch [650/50000], Loss: 2.9083\n",
            "Epoch [660/50000], Loss: 2.9082\n",
            "Epoch [670/50000], Loss: 2.9081\n",
            "Epoch [680/50000], Loss: 2.9080\n",
            "Epoch [690/50000], Loss: 2.9079\n",
            "Epoch [700/50000], Loss: 2.9078\n",
            "Epoch [710/50000], Loss: 2.9076\n",
            "Epoch [720/50000], Loss: 2.9074\n",
            "Epoch [730/50000], Loss: 2.9072\n",
            "Epoch [740/50000], Loss: 2.9070\n",
            "Epoch [750/50000], Loss: 2.9068\n",
            "Epoch [760/50000], Loss: 2.9067\n",
            "Epoch [770/50000], Loss: 2.9066\n",
            "Epoch [780/50000], Loss: 2.9066\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.906569407660698\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 17 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 2307.8637\n",
            "Epoch [20/2000], Loss: 2289.9703\n",
            "Epoch [30/2000], Loss: 2270.1383\n",
            "Epoch [40/2000], Loss: 2248.7190\n",
            "Epoch [50/2000], Loss: 2226.0830\n",
            "Epoch [60/2000], Loss: 2202.5388\n",
            "Epoch [70/2000], Loss: 2178.3200\n",
            "Epoch [80/2000], Loss: 2153.5956\n",
            "Epoch [90/2000], Loss: 2128.4870\n",
            "Epoch [100/2000], Loss: 2103.0821\n",
            "Epoch [110/2000], Loss: 2077.4452\n",
            "Epoch [120/2000], Loss: 2051.6248\n",
            "Epoch [130/2000], Loss: 2025.6577\n",
            "Epoch [140/2000], Loss: 1999.5727\n",
            "Epoch [150/2000], Loss: 1973.3927\n",
            "Epoch [160/2000], Loss: 1947.1361\n",
            "Epoch [170/2000], Loss: 1920.8177\n",
            "Epoch [180/2000], Loss: 1894.4498\n",
            "Epoch [190/2000], Loss: 1868.0424\n",
            "Epoch [200/2000], Loss: 1841.6039\n",
            "Epoch [210/2000], Loss: 1815.1409\n",
            "Epoch [220/2000], Loss: 1788.6591\n",
            "Epoch [230/2000], Loss: 1762.1627\n",
            "Epoch [240/2000], Loss: 1735.6549\n",
            "Epoch [250/2000], Loss: 1709.1382\n",
            "Epoch [260/2000], Loss: 1682.6140\n",
            "Epoch [270/2000], Loss: 1656.0830\n",
            "Epoch [280/2000], Loss: 1629.5449\n",
            "Epoch [290/2000], Loss: 1602.9994\n",
            "Epoch [300/2000], Loss: 1576.4454\n",
            "Epoch [310/2000], Loss: 1549.8823\n",
            "Epoch [320/2000], Loss: 1523.3098\n",
            "Epoch [330/2000], Loss: 1496.7286\n",
            "Epoch [340/2000], Loss: 1470.1403\n",
            "Epoch [350/2000], Loss: 1443.5477\n",
            "Epoch [360/2000], Loss: 1416.9542\n",
            "Epoch [370/2000], Loss: 1390.3635\n",
            "Epoch [380/2000], Loss: 1363.7795\n",
            "Epoch [390/2000], Loss: 1337.2056\n",
            "Epoch [400/2000], Loss: 1310.6448\n",
            "Epoch [410/2000], Loss: 1284.0996\n",
            "Epoch [420/2000], Loss: 1257.5722\n",
            "Epoch [430/2000], Loss: 1231.0644\n",
            "Epoch [440/2000], Loss: 1204.5775\n",
            "Epoch [450/2000], Loss: 1178.1130\n",
            "Epoch [460/2000], Loss: 1151.6718\n",
            "Epoch [470/2000], Loss: 1125.2549\n",
            "Epoch [480/2000], Loss: 1098.8633\n",
            "Epoch [490/2000], Loss: 1072.4978\n",
            "Epoch [500/2000], Loss: 1046.1592\n",
            "Epoch [510/2000], Loss: 1019.8482\n",
            "Epoch [520/2000], Loss: 993.5657\n",
            "Epoch [530/2000], Loss: 967.3125\n",
            "Epoch [540/2000], Loss: 941.0894\n",
            "Epoch [550/2000], Loss: 914.8974\n",
            "Epoch [560/2000], Loss: 888.7374\n",
            "Epoch [570/2000], Loss: 862.6103\n",
            "Epoch [580/2000], Loss: 836.5174\n",
            "Epoch [590/2000], Loss: 810.4599\n",
            "Epoch [600/2000], Loss: 784.4390\n",
            "Epoch [610/2000], Loss: 758.4563\n",
            "Epoch [620/2000], Loss: 732.5136\n",
            "Epoch [630/2000], Loss: 706.6127\n",
            "Epoch [640/2000], Loss: 680.7558\n",
            "Epoch [650/2000], Loss: 654.9455\n",
            "Epoch [660/2000], Loss: 629.1847\n",
            "Epoch [670/2000], Loss: 603.4768\n",
            "Epoch [680/2000], Loss: 577.8258\n",
            "Epoch [690/2000], Loss: 552.2366\n",
            "Epoch [700/2000], Loss: 526.7147\n",
            "Epoch [710/2000], Loss: 501.2671\n",
            "Epoch [720/2000], Loss: 475.9019\n",
            "Epoch [730/2000], Loss: 450.6294\n",
            "Epoch [740/2000], Loss: 425.4623\n",
            "Epoch [750/2000], Loss: 400.4165\n",
            "Epoch [760/2000], Loss: 375.5122\n",
            "Epoch [770/2000], Loss: 350.7757\n",
            "Epoch [780/2000], Loss: 326.2411\n",
            "Epoch [790/2000], Loss: 301.9536\n",
            "Epoch [800/2000], Loss: 277.9733\n",
            "Epoch [810/2000], Loss: 254.3821\n",
            "Epoch [820/2000], Loss: 231.2927\n",
            "Epoch [830/2000], Loss: 208.8625\n",
            "Epoch [840/2000], Loss: 187.3157\n",
            "Epoch [850/2000], Loss: 166.9776\n",
            "Epoch [860/2000], Loss: 148.3256\n",
            "Epoch [870/2000], Loss: 132.0541\n",
            "Epoch [880/2000], Loss: 119.1098\n",
            "Epoch [890/2000], Loss: 110.5614\n",
            "Epoch [900/2000], Loss: 107.0585\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.1291\n",
            "Epoch [20/50000], Loss: 3.1116\n",
            "Epoch [30/50000], Loss: 3.0937\n",
            "Epoch [40/50000], Loss: 3.0760\n",
            "Epoch [50/50000], Loss: 3.0591\n",
            "Epoch [60/50000], Loss: 3.0435\n",
            "Epoch [70/50000], Loss: 3.0293\n",
            "Epoch [80/50000], Loss: 3.0167\n",
            "Epoch [90/50000], Loss: 3.0056\n",
            "Epoch [100/50000], Loss: 2.9961\n",
            "Epoch [110/50000], Loss: 2.9880\n",
            "Epoch [120/50000], Loss: 2.9812\n",
            "Epoch [130/50000], Loss: 2.9754\n",
            "Epoch [140/50000], Loss: 2.9705\n",
            "Epoch [150/50000], Loss: 2.9662\n",
            "Epoch [160/50000], Loss: 2.9624\n",
            "Epoch [170/50000], Loss: 2.9590\n",
            "Epoch [180/50000], Loss: 2.9557\n",
            "Epoch [190/50000], Loss: 2.9526\n",
            "Epoch [200/50000], Loss: 2.9497\n",
            "Epoch [210/50000], Loss: 2.9469\n",
            "Epoch [220/50000], Loss: 2.9442\n",
            "Epoch [230/50000], Loss: 2.9417\n",
            "Epoch [240/50000], Loss: 2.9394\n",
            "Epoch [250/50000], Loss: 2.9373\n",
            "Epoch [260/50000], Loss: 2.9354\n",
            "Epoch [270/50000], Loss: 2.9337\n",
            "Epoch [280/50000], Loss: 2.9321\n",
            "Epoch [290/50000], Loss: 2.9307\n",
            "Epoch [300/50000], Loss: 2.9295\n",
            "Epoch [310/50000], Loss: 2.9283\n",
            "Epoch [320/50000], Loss: 2.9273\n",
            "Epoch [330/50000], Loss: 2.9263\n",
            "Epoch [340/50000], Loss: 2.9254\n",
            "Epoch [350/50000], Loss: 2.9244\n",
            "Epoch [360/50000], Loss: 2.9235\n",
            "Epoch [370/50000], Loss: 2.9226\n",
            "Epoch [380/50000], Loss: 2.9217\n",
            "Epoch [390/50000], Loss: 2.9208\n",
            "Epoch [400/50000], Loss: 2.9200\n",
            "Epoch [410/50000], Loss: 2.9191\n",
            "Epoch [420/50000], Loss: 2.9183\n",
            "Epoch [430/50000], Loss: 2.9174\n",
            "Epoch [440/50000], Loss: 2.9167\n",
            "Epoch [450/50000], Loss: 2.9159\n",
            "Epoch [460/50000], Loss: 2.9152\n",
            "Epoch [470/50000], Loss: 2.9146\n",
            "Epoch [480/50000], Loss: 2.9140\n",
            "Epoch [490/50000], Loss: 2.9134\n",
            "Epoch [500/50000], Loss: 2.9129\n",
            "Epoch [510/50000], Loss: 2.9124\n",
            "Epoch [520/50000], Loss: 2.9120\n",
            "Epoch [530/50000], Loss: 2.9115\n",
            "Epoch [540/50000], Loss: 2.9111\n",
            "Epoch [550/50000], Loss: 2.9107\n",
            "Epoch [560/50000], Loss: 2.9104\n",
            "Epoch [570/50000], Loss: 2.9101\n",
            "Epoch [580/50000], Loss: 2.9098\n",
            "Epoch [590/50000], Loss: 2.9095\n",
            "Epoch [600/50000], Loss: 2.9093\n",
            "Epoch [610/50000], Loss: 2.9091\n",
            "Epoch [620/50000], Loss: 2.9090\n",
            "Epoch [630/50000], Loss: 2.9089\n",
            "Epoch [640/50000], Loss: 2.9088\n",
            "Epoch [650/50000], Loss: 2.9086\n",
            "Epoch [660/50000], Loss: 2.9085\n",
            "Epoch [670/50000], Loss: 2.9084\n",
            "Epoch [680/50000], Loss: 2.9082\n",
            "Epoch [690/50000], Loss: 2.9080\n",
            "Epoch [700/50000], Loss: 2.9079\n",
            "Epoch [710/50000], Loss: 2.9077\n",
            "Epoch [720/50000], Loss: 2.9076\n",
            "Epoch [730/50000], Loss: 2.9074\n",
            "Epoch [740/50000], Loss: 2.9073\n",
            "Epoch [750/50000], Loss: 2.9073\n",
            "Epoch [760/50000], Loss: 2.9072\n",
            "Epoch [770/50000], Loss: 2.9072\n",
            "Epoch [780/50000], Loss: 2.9072\n",
            "Epoch [790/50000], Loss: 2.9072\n",
            "Epoch [800/50000], Loss: 2.9071\n",
            "Epoch [810/50000], Loss: 2.9071\n",
            "Epoch [820/50000], Loss: 2.9070\n",
            "Epoch [830/50000], Loss: 2.9068\n",
            "Epoch [840/50000], Loss: 2.9067\n",
            "Epoch [850/50000], Loss: 2.9065\n",
            "Epoch [860/50000], Loss: 2.9064\n",
            "Epoch [870/50000], Loss: 2.9063\n",
            "Epoch [880/50000], Loss: 2.9062\n",
            "Epoch [890/50000], Loss: 2.9061\n",
            "Epoch [900/50000], Loss: 2.9061\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9060994642876037\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 18 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 437.6160\n",
            "Epoch [20/2000], Loss: 420.1178\n",
            "Epoch [30/2000], Loss: 400.8103\n",
            "Epoch [40/2000], Loss: 380.0785\n",
            "Epoch [50/2000], Loss: 358.3320\n",
            "Epoch [60/2000], Loss: 335.9280\n",
            "Epoch [70/2000], Loss: 313.1625\n",
            "Epoch [80/2000], Loss: 290.2871\n",
            "Epoch [90/2000], Loss: 267.5343\n",
            "Epoch [100/2000], Loss: 245.1438\n",
            "Epoch [110/2000], Loss: 223.3920\n",
            "Epoch [120/2000], Loss: 202.6284\n",
            "Epoch [130/2000], Loss: 183.3247\n",
            "Epoch [140/2000], Loss: 166.1343\n",
            "Epoch [150/2000], Loss: 151.9495\n",
            "Epoch [160/2000], Loss: 141.8158\n",
            "Epoch [170/2000], Loss: 136.2674\n",
            "Epoch [180/2000], Loss: 134.4261\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.1591\n",
            "Epoch [20/50000], Loss: 3.1394\n",
            "Epoch [30/50000], Loss: 3.1190\n",
            "Epoch [40/50000], Loss: 3.0987\n",
            "Epoch [50/50000], Loss: 3.0793\n",
            "Epoch [60/50000], Loss: 3.0613\n",
            "Epoch [70/50000], Loss: 3.0449\n",
            "Epoch [80/50000], Loss: 3.0302\n",
            "Epoch [90/50000], Loss: 3.0173\n",
            "Epoch [100/50000], Loss: 3.0060\n",
            "Epoch [110/50000], Loss: 2.9962\n",
            "Epoch [120/50000], Loss: 2.9879\n",
            "Epoch [130/50000], Loss: 2.9808\n",
            "Epoch [140/50000], Loss: 2.9748\n",
            "Epoch [150/50000], Loss: 2.9698\n",
            "Epoch [160/50000], Loss: 2.9656\n",
            "Epoch [170/50000], Loss: 2.9619\n",
            "Epoch [180/50000], Loss: 2.9587\n",
            "Epoch [190/50000], Loss: 2.9558\n",
            "Epoch [200/50000], Loss: 2.9530\n",
            "Epoch [210/50000], Loss: 2.9504\n",
            "Epoch [220/50000], Loss: 2.9479\n",
            "Epoch [230/50000], Loss: 2.9454\n",
            "Epoch [240/50000], Loss: 2.9431\n",
            "Epoch [250/50000], Loss: 2.9408\n",
            "Epoch [260/50000], Loss: 2.9387\n",
            "Epoch [270/50000], Loss: 2.9367\n",
            "Epoch [280/50000], Loss: 2.9349\n",
            "Epoch [290/50000], Loss: 2.9332\n",
            "Epoch [300/50000], Loss: 2.9317\n",
            "Epoch [310/50000], Loss: 2.9304\n",
            "Epoch [320/50000], Loss: 2.9291\n",
            "Epoch [330/50000], Loss: 2.9280\n",
            "Epoch [340/50000], Loss: 2.9270\n",
            "Epoch [350/50000], Loss: 2.9260\n",
            "Epoch [360/50000], Loss: 2.9251\n",
            "Epoch [370/50000], Loss: 2.9242\n",
            "Epoch [380/50000], Loss: 2.9234\n",
            "Epoch [390/50000], Loss: 2.9226\n",
            "Epoch [400/50000], Loss: 2.9217\n",
            "Epoch [410/50000], Loss: 2.9209\n",
            "Epoch [420/50000], Loss: 2.9201\n",
            "Epoch [430/50000], Loss: 2.9193\n",
            "Epoch [440/50000], Loss: 2.9185\n",
            "Epoch [450/50000], Loss: 2.9178\n",
            "Epoch [460/50000], Loss: 2.9171\n",
            "Epoch [470/50000], Loss: 2.9165\n",
            "Epoch [480/50000], Loss: 2.9159\n",
            "Epoch [490/50000], Loss: 2.9154\n",
            "Epoch [500/50000], Loss: 2.9149\n",
            "Epoch [510/50000], Loss: 2.9144\n",
            "Epoch [520/50000], Loss: 2.9140\n",
            "Epoch [530/50000], Loss: 2.9136\n",
            "Epoch [540/50000], Loss: 2.9132\n",
            "Epoch [550/50000], Loss: 2.9128\n",
            "Epoch [560/50000], Loss: 2.9123\n",
            "Epoch [570/50000], Loss: 2.9119\n",
            "Epoch [580/50000], Loss: 2.9115\n",
            "Epoch [590/50000], Loss: 2.9112\n",
            "Epoch [600/50000], Loss: 2.9108\n",
            "Epoch [610/50000], Loss: 2.9106\n",
            "Epoch [620/50000], Loss: 2.9103\n",
            "Epoch [630/50000], Loss: 2.9102\n",
            "Epoch [640/50000], Loss: 2.9100\n",
            "Epoch [650/50000], Loss: 2.9099\n",
            "Epoch [660/50000], Loss: 2.9097\n",
            "Epoch [670/50000], Loss: 2.9096\n",
            "Epoch [680/50000], Loss: 2.9094\n",
            "Epoch [690/50000], Loss: 2.9092\n",
            "Epoch [700/50000], Loss: 2.9089\n",
            "Epoch [710/50000], Loss: 2.9087\n",
            "Epoch [720/50000], Loss: 2.9085\n",
            "Epoch [730/50000], Loss: 2.9083\n",
            "Epoch [740/50000], Loss: 2.9081\n",
            "Epoch [750/50000], Loss: 2.9080\n",
            "Epoch [760/50000], Loss: 2.9080\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.907988904012408\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 19 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 1706.4706\n",
            "Epoch [20/2000], Loss: 1687.7557\n",
            "Epoch [30/2000], Loss: 1667.0136\n",
            "Epoch [40/2000], Loss: 1644.6120\n",
            "Epoch [50/2000], Loss: 1620.9387\n",
            "Epoch [60/2000], Loss: 1596.3168\n",
            "Epoch [70/2000], Loss: 1570.9906\n",
            "Epoch [80/2000], Loss: 1545.1373\n",
            "Epoch [90/2000], Loss: 1518.8843\n",
            "Epoch [100/2000], Loss: 1492.3237\n",
            "Epoch [110/2000], Loss: 1465.5232\n",
            "Epoch [120/2000], Loss: 1438.5337\n",
            "Epoch [130/2000], Loss: 1411.3945\n",
            "Epoch [140/2000], Loss: 1384.1361\n",
            "Epoch [150/2000], Loss: 1356.7829\n",
            "Epoch [160/2000], Loss: 1329.3549\n",
            "Epoch [170/2000], Loss: 1301.8684\n",
            "Epoch [180/2000], Loss: 1274.3370\n",
            "Epoch [190/2000], Loss: 1246.7725\n",
            "Epoch [200/2000], Loss: 1219.1845\n",
            "Epoch [210/2000], Loss: 1191.5815\n",
            "Epoch [220/2000], Loss: 1163.9709\n",
            "Epoch [230/2000], Loss: 1136.3590\n",
            "Epoch [240/2000], Loss: 1108.7511\n",
            "Epoch [250/2000], Loss: 1081.1519\n",
            "Epoch [260/2000], Loss: 1053.5653\n",
            "Epoch [270/2000], Loss: 1025.9943\n",
            "Epoch [280/2000], Loss: 998.4415\n",
            "Epoch [290/2000], Loss: 970.9087\n",
            "Epoch [300/2000], Loss: 943.3967\n",
            "Epoch [310/2000], Loss: 915.9060\n",
            "Epoch [320/2000], Loss: 888.4360\n",
            "Epoch [330/2000], Loss: 860.9855\n",
            "Epoch [340/2000], Loss: 833.5525\n",
            "Epoch [350/2000], Loss: 806.1347\n",
            "Epoch [360/2000], Loss: 778.7291\n",
            "Epoch [370/2000], Loss: 751.3330\n",
            "Epoch [380/2000], Loss: 723.9445\n",
            "Epoch [390/2000], Loss: 696.5630\n",
            "Epoch [400/2000], Loss: 669.1899\n",
            "Epoch [410/2000], Loss: 641.8290\n",
            "Epoch [420/2000], Loss: 614.4866\n",
            "Epoch [430/2000], Loss: 587.1713\n",
            "Epoch [440/2000], Loss: 559.8935\n",
            "Epoch [450/2000], Loss: 532.6652\n",
            "Epoch [460/2000], Loss: 505.4995\n",
            "Epoch [470/2000], Loss: 478.4110\n",
            "Epoch [480/2000], Loss: 451.4160\n",
            "Epoch [490/2000], Loss: 424.5336\n",
            "Epoch [500/2000], Loss: 397.7862\n",
            "Epoch [510/2000], Loss: 371.2017\n",
            "Epoch [520/2000], Loss: 344.8154\n",
            "Epoch [530/2000], Loss: 318.6735\n",
            "Epoch [540/2000], Loss: 292.8375\n",
            "Epoch [550/2000], Loss: 267.3915\n",
            "Epoch [560/2000], Loss: 242.4527\n",
            "Epoch [570/2000], Loss: 218.1886\n",
            "Epoch [580/2000], Loss: 194.8431\n",
            "Epoch [590/2000], Loss: 172.7799\n",
            "Epoch [600/2000], Loss: 152.5510\n",
            "Epoch [610/2000], Loss: 134.9874\n",
            "Epoch [620/2000], Loss: 121.2313\n",
            "Epoch [630/2000], Loss: 112.5088\n",
            "Epoch [640/2000], Loss: 109.1837\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.1653\n",
            "Epoch [20/50000], Loss: 3.1455\n",
            "Epoch [30/50000], Loss: 3.1250\n",
            "Epoch [40/50000], Loss: 3.1045\n",
            "Epoch [50/50000], Loss: 3.0848\n",
            "Epoch [60/50000], Loss: 3.0664\n",
            "Epoch [70/50000], Loss: 3.0494\n",
            "Epoch [80/50000], Loss: 3.0342\n",
            "Epoch [90/50000], Loss: 3.0207\n",
            "Epoch [100/50000], Loss: 3.0088\n",
            "Epoch [110/50000], Loss: 2.9986\n",
            "Epoch [120/50000], Loss: 2.9899\n",
            "Epoch [130/50000], Loss: 2.9826\n",
            "Epoch [140/50000], Loss: 2.9764\n",
            "Epoch [150/50000], Loss: 2.9713\n",
            "Epoch [160/50000], Loss: 2.9670\n",
            "Epoch [170/50000], Loss: 2.9634\n",
            "Epoch [180/50000], Loss: 2.9602\n",
            "Epoch [190/50000], Loss: 2.9574\n",
            "Epoch [200/50000], Loss: 2.9548\n",
            "Epoch [210/50000], Loss: 2.9523\n",
            "Epoch [220/50000], Loss: 2.9500\n",
            "Epoch [230/50000], Loss: 2.9477\n",
            "Epoch [240/50000], Loss: 2.9454\n",
            "Epoch [250/50000], Loss: 2.9432\n",
            "Epoch [260/50000], Loss: 2.9411\n",
            "Epoch [270/50000], Loss: 2.9391\n",
            "Epoch [280/50000], Loss: 2.9373\n",
            "Epoch [290/50000], Loss: 2.9355\n",
            "Epoch [300/50000], Loss: 2.9339\n",
            "Epoch [310/50000], Loss: 2.9325\n",
            "Epoch [320/50000], Loss: 2.9312\n",
            "Epoch [330/50000], Loss: 2.9300\n",
            "Epoch [340/50000], Loss: 2.9289\n",
            "Epoch [350/50000], Loss: 2.9279\n",
            "Epoch [360/50000], Loss: 2.9271\n",
            "Epoch [370/50000], Loss: 2.9262\n",
            "Epoch [380/50000], Loss: 2.9255\n",
            "Epoch [390/50000], Loss: 2.9247\n",
            "Epoch [400/50000], Loss: 2.9240\n",
            "Epoch [410/50000], Loss: 2.9232\n",
            "Epoch [420/50000], Loss: 2.9225\n",
            "Epoch [430/50000], Loss: 2.9217\n",
            "Epoch [440/50000], Loss: 2.9210\n",
            "Epoch [450/50000], Loss: 2.9202\n",
            "Epoch [460/50000], Loss: 2.9194\n",
            "Epoch [470/50000], Loss: 2.9186\n",
            "Epoch [480/50000], Loss: 2.9178\n",
            "Epoch [490/50000], Loss: 2.9171\n",
            "Epoch [500/50000], Loss: 2.9163\n",
            "Epoch [510/50000], Loss: 2.9156\n",
            "Epoch [520/50000], Loss: 2.9150\n",
            "Epoch [530/50000], Loss: 2.9144\n",
            "Epoch [540/50000], Loss: 2.9138\n",
            "Epoch [550/50000], Loss: 2.9133\n",
            "Epoch [560/50000], Loss: 2.9128\n",
            "Epoch [570/50000], Loss: 2.9123\n",
            "Epoch [580/50000], Loss: 2.9119\n",
            "Epoch [590/50000], Loss: 2.9115\n",
            "Epoch [600/50000], Loss: 2.9111\n",
            "Epoch [610/50000], Loss: 2.9107\n",
            "Epoch [620/50000], Loss: 2.9104\n",
            "Epoch [630/50000], Loss: 2.9101\n",
            "Epoch [640/50000], Loss: 2.9097\n",
            "Epoch [650/50000], Loss: 2.9095\n",
            "Epoch [660/50000], Loss: 2.9092\n",
            "Epoch [670/50000], Loss: 2.9090\n",
            "Epoch [680/50000], Loss: 2.9089\n",
            "Epoch [690/50000], Loss: 2.9087\n",
            "Epoch [700/50000], Loss: 2.9086\n",
            "Epoch [710/50000], Loss: 2.9085\n",
            "Epoch [720/50000], Loss: 2.9085\n",
            "Epoch [730/50000], Loss: 2.9084\n",
            "Epoch [740/50000], Loss: 2.9083\n",
            "Epoch [750/50000], Loss: 2.9083\n",
            "Epoch [760/50000], Loss: 2.9082\n",
            "Epoch [770/50000], Loss: 2.9080\n",
            "Epoch [780/50000], Loss: 2.9079\n",
            "Epoch [790/50000], Loss: 2.9077\n",
            "Epoch [800/50000], Loss: 2.9076\n",
            "Epoch [810/50000], Loss: 2.9075\n",
            "Epoch [820/50000], Loss: 2.9074\n",
            "Epoch [830/50000], Loss: 2.9073\n",
            "Epoch [840/50000], Loss: 2.9072\n",
            "Epoch [850/50000], Loss: 2.9072\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9072135583796\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 20 Start Searching------\n",
            "-----Consumer Type Search Begin-----\n",
            "-----Rule Weight Optimization-----\n",
            "Epoch [10/2000], Loss: 602.3891\n",
            "Epoch [20/2000], Loss: 584.8823\n",
            "Epoch [30/2000], Loss: 565.5683\n",
            "Epoch [40/2000], Loss: 544.8282\n",
            "Epoch [50/2000], Loss: 523.0632\n",
            "Epoch [60/2000], Loss: 500.6155\n",
            "Epoch [70/2000], Loss: 477.7566\n",
            "Epoch [80/2000], Loss: 454.7001\n",
            "Epoch [90/2000], Loss: 431.6209\n",
            "Epoch [100/2000], Loss: 408.6729\n",
            "Epoch [110/2000], Loss: 386.0037\n",
            "Epoch [120/2000], Loss: 363.7667\n",
            "Epoch [130/2000], Loss: 342.1311\n",
            "Epoch [140/2000], Loss: 321.2937\n",
            "Epoch [150/2000], Loss: 301.4998\n",
            "Epoch [160/2000], Loss: 283.0588\n",
            "Epoch [170/2000], Loss: 266.3810\n",
            "Epoch [180/2000], Loss: 251.9358\n",
            "Epoch [190/2000], Loss: 240.1742\n",
            "Epoch [200/2000], Loss: 231.5569\n",
            "Epoch [210/2000], Loss: 226.4720\n",
            "Epoch [220/2000], Loss: 224.3551\n",
            "-----Consumer Type Search End-----\n",
            "-----Proportion Update Search Begin-----\n",
            "Epoch [10/50000], Loss: 3.2016\n",
            "Epoch [20/50000], Loss: 3.1795\n",
            "Epoch [30/50000], Loss: 3.1564\n",
            "Epoch [40/50000], Loss: 3.1331\n",
            "Epoch [50/50000], Loss: 3.1106\n",
            "Epoch [60/50000], Loss: 3.0894\n",
            "Epoch [70/50000], Loss: 3.0699\n",
            "Epoch [80/50000], Loss: 3.0522\n",
            "Epoch [90/50000], Loss: 3.0365\n",
            "Epoch [100/50000], Loss: 3.0226\n",
            "Epoch [110/50000], Loss: 3.0105\n",
            "Epoch [120/50000], Loss: 3.0001\n",
            "Epoch [130/50000], Loss: 2.9911\n",
            "Epoch [140/50000], Loss: 2.9835\n",
            "Epoch [150/50000], Loss: 2.9770\n",
            "Epoch [160/50000], Loss: 2.9715\n",
            "Epoch [170/50000], Loss: 2.9668\n",
            "Epoch [180/50000], Loss: 2.9628\n",
            "Epoch [190/50000], Loss: 2.9594\n",
            "Epoch [200/50000], Loss: 2.9564\n",
            "Epoch [210/50000], Loss: 2.9538\n",
            "Epoch [220/50000], Loss: 2.9514\n",
            "Epoch [230/50000], Loss: 2.9491\n",
            "Epoch [240/50000], Loss: 2.9470\n",
            "Epoch [250/50000], Loss: 2.9449\n",
            "Epoch [260/50000], Loss: 2.9430\n",
            "Epoch [270/50000], Loss: 2.9411\n",
            "Epoch [280/50000], Loss: 2.9392\n",
            "Epoch [290/50000], Loss: 2.9374\n",
            "Epoch [300/50000], Loss: 2.9358\n",
            "Epoch [310/50000], Loss: 2.9342\n",
            "Epoch [320/50000], Loss: 2.9327\n",
            "Epoch [330/50000], Loss: 2.9314\n",
            "Epoch [340/50000], Loss: 2.9301\n",
            "Epoch [350/50000], Loss: 2.9290\n",
            "Epoch [360/50000], Loss: 2.9280\n",
            "Epoch [370/50000], Loss: 2.9270\n",
            "Epoch [380/50000], Loss: 2.9261\n",
            "Epoch [390/50000], Loss: 2.9253\n",
            "Epoch [400/50000], Loss: 2.9245\n",
            "Epoch [410/50000], Loss: 2.9237\n",
            "Epoch [420/50000], Loss: 2.9230\n",
            "Epoch [430/50000], Loss: 2.9222\n",
            "Epoch [440/50000], Loss: 2.9215\n",
            "Epoch [450/50000], Loss: 2.9208\n",
            "Epoch [460/50000], Loss: 2.9200\n",
            "Epoch [470/50000], Loss: 2.9193\n",
            "Epoch [480/50000], Loss: 2.9187\n",
            "Epoch [490/50000], Loss: 2.9180\n",
            "Epoch [500/50000], Loss: 2.9174\n",
            "Epoch [510/50000], Loss: 2.9169\n",
            "Epoch [520/50000], Loss: 2.9164\n",
            "Epoch [530/50000], Loss: 2.9159\n",
            "Epoch [540/50000], Loss: 2.9155\n",
            "Epoch [550/50000], Loss: 2.9152\n",
            "Epoch [560/50000], Loss: 2.9148\n",
            "Epoch [570/50000], Loss: 2.9145\n",
            "Epoch [580/50000], Loss: 2.9141\n",
            "Epoch [590/50000], Loss: 2.9137\n",
            "Epoch [600/50000], Loss: 2.9133\n",
            "Epoch [610/50000], Loss: 2.9129\n",
            "Epoch [620/50000], Loss: 2.9124\n",
            "Epoch [630/50000], Loss: 2.9120\n",
            "Epoch [640/50000], Loss: 2.9116\n",
            "Epoch [650/50000], Loss: 2.9113\n",
            "Epoch [660/50000], Loss: 2.9110\n",
            "Epoch [670/50000], Loss: 2.9107\n",
            "Epoch [680/50000], Loss: 2.9105\n",
            "Epoch [690/50000], Loss: 2.9104\n",
            "Epoch [700/50000], Loss: 2.9102\n",
            "Epoch [710/50000], Loss: 2.9101\n",
            "Epoch [720/50000], Loss: 2.9100\n",
            "Epoch [730/50000], Loss: 2.9098\n",
            "Epoch [740/50000], Loss: 2.9096\n",
            "Epoch [750/50000], Loss: 2.9094\n",
            "Epoch [760/50000], Loss: 2.9091\n",
            "Epoch [770/50000], Loss: 2.9089\n",
            "Epoch [780/50000], Loss: 2.9086\n",
            "Epoch [790/50000], Loss: 2.9084\n",
            "Epoch [800/50000], Loss: 2.9082\n",
            "Epoch [810/50000], Loss: 2.9081\n",
            "Epoch [820/50000], Loss: 2.9080\n",
            "-----Proportion Update Search End-----\n",
            "main problem loss:  2.9080300890570614\n",
            "-----One Iteration Done-----\n",
            "-----Consumer Type 1------\n",
            "Consumer Proportion: 0.07025871037050847\n",
            "-----Consumer Type 2------\n",
            "Consumer Proportion: 0.19974494647822644\n",
            "-----Consumer Type 3------\n",
            "Consumer Proportion: 0.06240645414464554\n",
            "-----Consumer Type 4------\n",
            "Consumer Proportion: 0.1957426573695083\n",
            "-----Consumer Type 5------\n",
            "Consumer Proportion: 7.248993917314819e-06\n",
            "-----Consumer Type 6------\n",
            "Consumer Proportion: 6.6249105709242145e-06\n",
            "-----Consumer Type 7------\n",
            "Consumer Proportion: 0.22270970132069243\n",
            "-----Consumer Type 8------\n",
            "Consumer Proportion: 0.22247261620976644\n",
            "-----Consumer Type 9------\n",
            "Consumer Proportion: 0.0002454924123662921\n",
            "-----Consumer Type 10------\n",
            "Consumer Proportion: 5.741496755496612e-06\n",
            "-----Consumer Type 11------\n",
            "Consumer Proportion: 7.038990607479914e-06\n",
            "-----Consumer Type 12------\n",
            "Consumer Proportion: 0.00013524162696158017\n",
            "-----Consumer Type 13------\n",
            "Consumer Proportion: 7.011420412566142e-06\n",
            "-----Consumer Type 14------\n",
            "Consumer Proportion: 0.015909659159326784\n",
            "-----Consumer Type 15------\n",
            "Consumer Proportion: 0.0103000982593676\n",
            "-----Consumer Type 16------\n",
            "Consumer Proportion: 6.997600624172462e-06\n",
            "-----Consumer Type 17------\n",
            "Consumer Proportion: 6.6602456590483375e-06\n",
            "-----Consumer Type 18------\n",
            "Consumer Proportion: 7.295855033495623e-06\n",
            "-----Consumer Type 19------\n",
            "Consumer Proportion: 6.549398751036157e-06\n",
            "-----Consumer Type 20------\n",
            "Consumer Proportion: 7.1869188593864966e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WaN7HitqTuZT"
      },
      "id": "WaN7HitqTuZT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}